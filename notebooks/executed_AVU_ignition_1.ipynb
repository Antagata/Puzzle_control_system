{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f665d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:52:56.735849Z",
     "iopub.status.busy": "2025-09-05T06:52:56.734849Z",
     "iopub.status.idle": "2025-09-05T06:52:56.739837Z",
     "shell.execute_reply": "2025-09-05T06:52:56.739837Z"
    },
    "papermill": {
     "duration": 0.006999,
     "end_time": "2025-09-05T06:52:56.740846",
     "exception": false,
     "start_time": "2025-09-05T06:52:56.733847",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_path = \"C:\\\\Users\\\\Marco.Africani\\\\OneDrive - AVU SA\\\\AVU CPI Campaign\\\\Puzzle_control_Reports\\\\SOURCE_FILES\"\n",
    "output_path = \"C:\\\\Users\\\\Marco.Africani\\\\OneDrive - AVU SA\\\\AVU CPI Campaign\\\\Puzzle_control_Reports\\\\IRON_DATA\"\n",
    "week_number = 36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecab83a5-9a4c-4ccc-b568-2c3a7e97689d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:52:56.742845Z",
     "iopub.status.busy": "2025-09-05T06:52:56.741843Z",
     "iopub.status.idle": "2025-09-05T06:52:57.679086Z",
     "shell.execute_reply": "2025-09-05T06:52:57.679086Z"
    },
    "papermill": {
     "duration": 0.93824,
     "end_time": "2025-09-05T06:52:57.679086",
     "exception": false,
     "start_time": "2025-09-05T06:52:56.740846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Source data path:   C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\SOURCE_FILES\n",
      "✅ Output data path:   C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\n",
      "📦 Locked weeks path:  C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\locked_weeks\n",
      "📄 Exports path:       C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\exports\n",
      "📊 PowerBI path:       C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi\n",
      "📅 Using ISO Week:     36 (2025-09-01 → 2025-09-07)\n",
      "🧩 Filters provided:   yes   → resolved: {'loyalty': 'all', 'wine_type': None, 'bottle_size': 750, 'price_tier_bucket': '', 'last_stock': True, 'last_stock_threshold': 10, 'seasonality_boost': False, 'style': 'default', 'calendar_day': None}\n",
      "🔒 Locked snapshot:    yes\n",
      "✅ Environment & parameters initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1 AVU_ignition_1.ipynb: Global Setup & Parameter Intake (Papermill/Env/UI-safe) ---\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Constants used across the run\n",
    "# -------------------------------\n",
    "NUM_SLOTS = 5\n",
    "DAYS_FULL = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "DAYS_LOWER = [d.lower() for d in DAYS_FULL]\n",
    "\n",
    "# When preferences can't be fully satisfied, later cells can consult these knobs\n",
    "ENGINE_FILL_POLICY = {\n",
    "    \"min_slots_per_day\": NUM_SLOTS,   # aim to fill all visible slots\n",
    "    \"relaxation_enabled\": True,\n",
    "    \"relaxation_order\": [\n",
    "        # later cells should progressively relax in this order until enough wines found:\n",
    "        \"loyalty\",          # treat 'all' if too strict\n",
    "        \"wine_type\",        # allow any type if filter blocks results\n",
    "        \"bottle_size\",      # allow any size if specific format too rare\n",
    "        \"price_tier\",       # widen tier ranges (Budget→Mid→Premium→Luxury→Ultra)\n",
    "        \"last_stock_only\",  # ignore last-stock constraint if needed\n",
    "        \"seasonality\"       # ignore seasonality boost as hard constraint (keep as soft weight)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Try to read Papermill parameters (if any)\n",
    "# -------------------------------\n",
    "_pm_params: Dict[str, Any] = {}\n",
    "try:\n",
    "    from papermill import get_parameters  # type: ignore\n",
    "    _pm_params = get_parameters() or {}\n",
    "except Exception:\n",
    "    _pm_params = {}\n",
    "\n",
    "def _coerce_int(value, fallback):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except Exception:\n",
    "        return int(fallback)\n",
    "\n",
    "def _iso_week_now():\n",
    "    try:\n",
    "        return datetime.now().isocalendar().week\n",
    "    except Exception:\n",
    "        return int(datetime.now().strftime(\"%V\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Resolve week_number (priority: papermill -> env -> runtime default)\n",
    "# -------------------------------\n",
    "if \"week_number\" in globals():\n",
    "    _week_candidate = globals().get(\"week_number\")\n",
    "elif \"week_number\" in _pm_params:\n",
    "    _week_candidate = _pm_params.get(\"week_number\")\n",
    "else:\n",
    "    _week_candidate = os.getenv(\"WEEK_NUMBER\", _iso_week_now())\n",
    "\n",
    "week_number = _coerce_int(_week_candidate, _iso_week_now())\n",
    "# Clamp to ISO range\n",
    "if week_number < 1 or week_number > 53:\n",
    "    week_number = _iso_week_now()\n",
    "\n",
    "# Useful week helpers\n",
    "def week_bounds(iso_week: int, year: int = datetime.now().year):\n",
    "    # Monday as first day of ISO week\n",
    "    d = date.fromisocalendar(year, iso_week, 1)\n",
    "    return d, d + timedelta(days=6)\n",
    "\n",
    "week_start_date, week_end_date = week_bounds(week_number)\n",
    "\n",
    "# --- YEAR AWARENESS + EUROPE/ZURICH TZ + PARAM SUPPORT ---\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "def _iso_week_now_europe():\n",
    "    now = datetime.now(ZoneInfo(\"Europe/Zurich\"))\n",
    "    iso = now.isocalendar()\n",
    "    return iso.year, iso.week\n",
    "\n",
    "# Allow papermill/env to pass an explicit calendar year\n",
    "if \"calendar_year\" in globals():\n",
    "    _year_candidate = globals().get(\"calendar_year\")\n",
    "elif \"calendar_year\" in _pm_params:\n",
    "    _year_candidate = _pm_params.get(\"calendar_year\")\n",
    "else:\n",
    "    _year_candidate = os.getenv(\"CALENDAR_YEAR\", None)\n",
    "\n",
    "try:\n",
    "    calendar_year = int(_year_candidate) if _year_candidate is not None else _iso_week_now_europe()[0]\n",
    "except Exception:\n",
    "    calendar_year = _iso_week_now_europe()[0]\n",
    "\n",
    "# Week bounds should use the chosen calendar_year\n",
    "def week_bounds(iso_week: int, year: int):\n",
    "    # Guard invalid 53rd week for some years\n",
    "    try:\n",
    "        d = date.fromisocalendar(year, iso_week, 1)\n",
    "    except ValueError:\n",
    "        iso_week = min(max(iso_week, 1), 52)\n",
    "        d = date.fromisocalendar(year, iso_week, 1)\n",
    "    return d, d + timedelta(days=6)\n",
    "\n",
    "week_start_date, week_end_date = week_bounds(week_number, calendar_year)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Resolve IO paths (priority: papermill -> env -> sensible defaults)\n",
    "# -------------------------------\n",
    "def _resolve_path(param_key, env_key, default_path_str):\n",
    "    if param_key in globals():\n",
    "        p = globals().get(param_key)\n",
    "    elif param_key in _pm_params:\n",
    "        p = _pm_params.get(param_key)\n",
    "    else:\n",
    "        p = os.getenv(env_key, default_path_str)\n",
    "    return Path(p)\n",
    "\n",
    "# Defaults match your project structure\n",
    "_default_source = str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"SOURCE_FILES\")\n",
    "_default_output = str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\")\n",
    "\n",
    "SOURCE_PATH: Path = _resolve_path(\"input_path\", \"INPUT_PATH\", _default_source)\n",
    "OUTPUT_PATH: Path = _resolve_path(\"output_path\", \"OUTPUT_PATH\", _default_output)\n",
    "\n",
    "# Make sure all dirs exist (incl. subfolders we’ll use)\n",
    "SOURCE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "LOCKED_PATH: Path = OUTPUT_PATH / \"locked_weeks\"\n",
    "CALENDAR_PATH: Path = OUTPUT_PATH / \"calendar\"       # where schedule JSONs per week can live\n",
    "EXPORTS_PATH: Path = OUTPUT_PATH / \"exports\"         # generic exports\n",
    "POWERBI_PATH: Path = OUTPUT_PATH / \"powerbi\"         # enriched Excel for Power BI\n",
    "TMP_PATH: Path = OUTPUT_PATH / \"_tmp\"\n",
    "\n",
    "for p in (LOCKED_PATH, CALENDAR_PATH, EXPORTS_PATH, POWERBI_PATH, TMP_PATH):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional reports path (kept for compatibility)\n",
    "REPORTS_PATH: Path = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"non_recipient_reports\"\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Load lightweight UI inputs if present (non-fatal if missing)\n",
    "#    These files are written by the Flask app before launching the notebook.\n",
    "# -------------------------------\n",
    "NOTEBOOKS_DIR = Path(\"notebooks\")\n",
    "FILTERS_FILE = NOTEBOOKS_DIR / \"filters.json\"\n",
    "LOCKED_SNAPSHOT_FILE = NOTEBOOKS_DIR / \"locked_calendar.json\"  # snapshot of UI locks passed in\n",
    "\n",
    "def _load_json_or_empty(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "raw_filters: Dict[str, Any] = _load_json_or_empty(FILTERS_FILE)              # may be {}\n",
    "_locked_raw = _load_json_or_empty(LOCKED_SNAPSHOT_FILE)                       # may be {}\n",
    "\n",
    "# -------- Canonicalize UI/legacy locks to one shape --------\n",
    "# Accepts:\n",
    "#   - {\"weekly_calendar\": {day: [slot...]}}\n",
    "#   - {day: [slot...]}\n",
    "#   - {day: {\"main\":[slot...], \"overflow\":[...]}}\n",
    "# Returns:\n",
    "#   {day: {\"main\": [None-padded to NUM_SLOTS], \"overflow\": list}}\n",
    "def _coerce_locked_snapshot(snap, days=None, num_slots=NUM_SLOTS):\n",
    "    if days is None:\n",
    "        days = DAYS_FULL\n",
    "\n",
    "    # unwrap {\"weekly_calendar\": {...}} if present\n",
    "    if isinstance(snap, dict) and isinstance(snap.get(\"weekly_calendar\"), dict):\n",
    "        snap = snap[\"weekly_calendar\"]\n",
    "\n",
    "    out = {}\n",
    "    for d in days:\n",
    "        node = (snap or {}).get(d) or (snap or {}).get(d.lower())\n",
    "        main, overflow = [], []\n",
    "        if isinstance(node, list):\n",
    "            main = node\n",
    "        elif isinstance(node, dict):\n",
    "            if isinstance(node.get(\"main\"), list):\n",
    "                main = node.get(\"main\") or []\n",
    "            elif isinstance(node.get(\"slots\"), list):\n",
    "                main = node.get(\"slots\") or []\n",
    "            else:\n",
    "                # tolerate indexed dicts {0:...,1:...}\n",
    "                tmp = []\n",
    "                for i in range(num_slots):\n",
    "                    v = node.get(i, node.get(str(i)))\n",
    "                    tmp.append(v if v else None)\n",
    "                main = tmp\n",
    "            if isinstance(node.get(\"overflow\"), list):\n",
    "                overflow = node.get(\"overflow\") or []\n",
    "        # clamp & pad main\n",
    "        main = (main or [])[:num_slots]\n",
    "        if len(main) < num_slots:\n",
    "            main = main + [None] * (num_slots - len(main))\n",
    "        out[d] = {\"main\": [x if x else None for x in main], \"overflow\": list(overflow or [])}\n",
    "    return out\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Normalize the incoming UI snapshot\n",
    "LOCKED_SNAPSHOT_NORM = _coerce_locked_snapshot(_locked_raw, DAYS_FULL, NUM_SLOTS)\n",
    "\n",
    "# If empty, fallback to persisted week file\n",
    "def _is_empty_main(snap_norm):\n",
    "    return not any(any(snap_norm[d][\"main\"]) for d in DAYS_FULL)\n",
    "\n",
    "if _is_empty_main(LOCKED_SNAPSHOT_NORM):\n",
    "    persisted = _load_json(LOCKED_PATH / f\"locked_calendar_{calendar_year}_week_{week_number}.json\")\n",
    "    LOCKED_SNAPSHOT_NORM = _coerce_locked_snapshot(persisted, DAYS_FULL, NUM_SLOTS)\n",
    "\n",
    "# Export *both* forms for downstream cells:\n",
    "# - EFFECTIVE_LOCKS / LOCKED_CALENDAR: {day: [slot,..]} (Cell 6/7 expect this)\n",
    "# - locked_calendar_snapshot: {day: {\"main\":[...], \"overflow\":[...]}} (for any structured readers)\n",
    "EFFECTIVE_LOCKS = {d: LOCKED_SNAPSHOT_NORM[d][\"main\"] for d in DAYS_FULL}\n",
    "LOCKED_CALENDAR = dict(EFFECTIVE_LOCKS)  # alias used by some cells\n",
    "locked_calendar_snapshot = dict(LOCKED_SNAPSHOT_NORM)  # keep structured version for completeness\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Helpers: normalize filters & day keys\n",
    "# -------------------------------\n",
    "PRICE_BUCKETS = [\"Budget\",\"Mid-range\",\"Premium\",\"Luxury\",\"Ultra Luxury\"]\n",
    "\n",
    "def price_tier_label(price: float) -> str:\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p < 50: return \"Budget\"\n",
    "    elif p < 100: return \"Mid-range\"\n",
    "    elif p < 200: return \"Premium\"\n",
    "    elif p < 500: return \"Luxury\"\n",
    "    else: return \"Ultra Luxury\"\n",
    "\n",
    "DEFAULT_FILTERS: Dict[str, Any] = {\n",
    "    \"loyalty\": \"all\",\n",
    "    \"wine_type\": None,        # None = all\n",
    "    \"bottle_size\": 750,       # ml\n",
    "    \"price_tier_bucket\": \"\",  # \"\" = all\n",
    "    \"last_stock\": False,\n",
    "    \"last_stock_threshold\": 10,\n",
    "    \"seasonality_boost\": False,\n",
    "    \"style\": \"default\",\n",
    "    \"calendar_day\": None\n",
    "}\n",
    "\n",
    "def _as_bool(x): \n",
    "    if isinstance(x, bool): return x\n",
    "    if isinstance(x, (int, float)): return bool(x)\n",
    "    if isinstance(x, str): return x.strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n",
    "    return False\n",
    "\n",
    "def _as_int(x, default=None):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def normalize_filters(f: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    f = dict(f or {})\n",
    "    out = dict(DEFAULT_FILTERS)\n",
    "\n",
    "    out[\"loyalty\"] = str(f.get(\"loyalty\", out[\"loyalty\"])).strip().lower() or \"all\"\n",
    "\n",
    "    wt = f.get(\"wine_type\", out[\"wine_type\"])\n",
    "    if isinstance(wt, str) and wt.strip().lower() == \"all\":\n",
    "        wt = None\n",
    "    out[\"wine_type\"] = wt\n",
    "\n",
    "    out[\"bottle_size\"] = _as_int(f.get(\"bottle_size\", out[\"bottle_size\"]), 750)\n",
    "\n",
    "    pt = str(f.get(\"price_tier_bucket\", out[\"price_tier_bucket\"])).strip()\n",
    "    out[\"price_tier_bucket\"] = pt if pt in PRICE_BUCKETS or pt == \"\" else \"\"\n",
    "\n",
    "    out[\"last_stock\"] = _as_bool(f.get(\"last_stock\", out[\"last_stock\"]))\n",
    "    out[\"last_stock_threshold\"] = _as_int(f.get(\"last_stock_threshold\", out[\"last_stock_threshold\"]), 10)\n",
    "\n",
    "    out[\"seasonality_boost\"] = _as_bool(f.get(\"seasonality_boost\", out[\"seasonality_boost\"]))\n",
    "\n",
    "    style = str(f.get(\"style\", out[\"style\"])).strip().lower()\n",
    "    out[\"style\"] = style if style in {\"default\",\"cat\",\"nigo\"} else \"default\"\n",
    "\n",
    "    cd = f.get(\"calendar_day\", None)\n",
    "    out[\"calendar_day\"] = str(cd) if cd else None\n",
    "\n",
    "    return out\n",
    "\n",
    "filters = normalize_filters(raw_filters)\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Echo configuration\n",
    "# -------------------------------\n",
    "print(f\"✅ Source data path:   {SOURCE_PATH.resolve()}\")\n",
    "print(f\"✅ Output data path:   {OUTPUT_PATH.resolve()}\")\n",
    "print(f\"📦 Locked weeks path:  {LOCKED_PATH.resolve()}\")\n",
    "print(f\"📄 Exports path:       {EXPORTS_PATH.resolve()}\")\n",
    "print(f\"📊 PowerBI path:       {POWERBI_PATH.resolve()}\")\n",
    "print(f\"📅 Using ISO Week:     {week_number} ({week_start_date} → {week_end_date})\")\n",
    "print(f\"🧩 Filters provided:   {'yes' if raw_filters else 'no'}   → resolved: {filters}\")\n",
    "print(f\"🔒 Locked snapshot:    {'yes' if any(EFFECTIVE_LOCKS.get(d) for d in DAYS_FULL) else 'no'}\")\n",
    "print(\"✅ Environment & parameters initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4669de-67cd-44d4-a71c-40d23ee0c3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:52:57.679086Z",
     "iopub.status.busy": "2025-09-05T06:52:57.679086Z",
     "iopub.status.idle": "2025-09-05T06:53:27.640861Z",
     "shell.execute_reply": "2025-09-05T06:53:27.640861Z"
    },
    "papermill": {
     "duration": 29.961775,
     "end_time": "2025-09-05T06:53:27.640861",
     "exception": false,
     "start_time": "2025-09-05T06:52:57.679086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HIST] Discovered 1 OMT files: ['OMT Main Offer List.xlsx']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ History map written → wine_campaign_history.json (keys: 53)\n",
      "🧭 campaign_index.json written (by_id / by_name_vintage).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Wrote 129 week files across 4 year(s).\n",
      "🔒 Locked calendars → locked_weeks\n",
      "🧹 Leads files created empty so leads boxes start blank each session.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: HISTORY - Campaign History Ingestion (OMT main offer list) ---\n",
    "# Builds history artifacts from OMT \"main offer list\" files and writes:\n",
    "#   - OUTPUT_PATH/history/wine_campaign_history.json\n",
    "#   - OUTPUT_PATH/campaign_index.json (by_id / by_name_vintage)\n",
    "#   - OUTPUT_PATH/weekly_campaign_schedule_week_{WEEK}.json (flat arrays)\n",
    "#   - OUTPUT_PATH/locked_weeks/locked_calendar_week_{WEEK}.json\n",
    "#   - OUTPUT_PATH/weekly_leads_week_{WEEK}.json\n",
    "#   - OUTPUT_PATH/schedule_index.json\n",
    "from datetime import datetime, date, timedelta\n",
    "import os, io, re, time, shutil, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "# --- Constants (shared with Cell 1) ---\n",
    "NUM_SLOTS = 5\n",
    "DAYS_FULL = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "DAYMAP = {1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thursday\",5:\"Friday\",6:\"Saturday\",7:\"Sunday\"}\n",
    "\n",
    "# --- Paths (from Cell 1) ---\n",
    "HISTORY_DIR = OUTPUT_PATH / \"history\"\n",
    "HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_JSON = HISTORY_DIR / \"wine_campaign_history.json\"\n",
    "\n",
    "LOCKED_PATH = OUTPUT_PATH / \"locked_weeks\"\n",
    "LOCKED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Year/Week alignment with UI selection\n",
    "CURRENT_YEAR = (week_start_date.year if \"week_start_date\" in globals() else datetime.now().year)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Discover OMT files (robust)\n",
    "# -------------------------------\n",
    "OMT_FILES = []\n",
    "cands = list(SOURCE_PATH.glob(\"**/*.xlsx\")) + list(SOURCE_PATH.glob(\"**/*.csv\"))\n",
    "for p in cands:\n",
    "    # accept common variations: OMT ... (main)? ... (offer|campaign) ... list\n",
    "    if re.search(r\"omt.*(main)?.*(offer|campaign).*(list)\", p.name, flags=re.I):\n",
    "        OMT_FILES.append(p)\n",
    "# Fallback: be a bit looser if nothing matched\n",
    "if not OMT_FILES:\n",
    "    for p in cands:\n",
    "        if re.search(r\"(offer|campaign).*(list)\", p.name, flags=re.I):\n",
    "            OMT_FILES.append(p)\n",
    "OMT_FILES = sorted(set(OMT_FILES))\n",
    "print(f\"[HIST] Discovered {len(OMT_FILES)} OMT files:\", [p.name for p in OMT_FILES] or \"—\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2) Safe readers (work around file locking)\n",
    "# -----------------------------------------\n",
    "def _safe_read_xlsx(p: Path) -> pd.DataFrame:\n",
    "    # Try read-bytes first (works even if file is open/locked sometimes)\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                buf = f.read()\n",
    "            return pd.read_excel(io.BytesIO(buf), engine=\"openpyxl\")\n",
    "        except PermissionError as e:\n",
    "            print(f\"⏳ PermissionError on {p.name} (attempt {attempt+1}/3): {e}\")\n",
    "            time.sleep(0.7)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Read-bytes failed on {p.name}: {e}\")\n",
    "            break  # if not permission, don't spin\n",
    "    # Fallback: copy to TMP and read\n",
    "    try:\n",
    "        tmp = TMP_PATH / f\"shadow_{int(time.time()*1000)}_{p.name}\"\n",
    "        shutil.copy2(p, tmp)\n",
    "        return pd.read_excel(tmp, engine=\"openpyxl\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fallback copy+read failed for {p.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _safe_read_csv(p: Path) -> pd.DataFrame:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            with open(p, \"rb\") as f:\n",
    "                buf = f.read()\n",
    "            return pd.read_csv(io.BytesIO(buf))\n",
    "        except PermissionError as e:\n",
    "            print(f\"⏳ PermissionError on {p.name} (attempt {attempt+1}/3): {e}\")\n",
    "            time.sleep(0.7)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Read-bytes failed on {p.name}: {e}\")\n",
    "            break\n",
    "    try:\n",
    "        tmp = TMP_PATH / f\"shadow_{int(time.time()*1000)}_{p.name}\"\n",
    "        shutil.copy2(p, tmp)\n",
    "        return pd.read_csv(tmp)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fallback copy+read failed for {p.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _load_omt_file(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = _safe_read_xlsx(p) if p.suffix.lower() == \".xlsx\" else _safe_read_csv(p)\n",
    "        if df.empty:\n",
    "            print(f\"⚠️ {p.name} loaded but EMPTY after safe read.\")\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {p.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --------------------------------\n",
    "# 3) Column normalization & parse\n",
    "# --------------------------------\n",
    "ALIASES = {\n",
    "    # datetime\n",
    "    \"Schedule DateTime\":\"schedule_dt\",\"Scheduled\":\"schedule_dt\",\"Scheduled At\":\"schedule_dt\",\n",
    "    \"DateTime\":\"schedule_dt\",\"schedule_datetime\":\"schedule_dt\",\n",
    "    \"Date\":\"schedule_dt\",\"Send Date\":\"schedule_dt\",\"Offer Date\":\"schedule_dt\",\n",
    "    \"Campaign Date\":\"schedule_dt\",\"Scheduled Date\":\"schedule_dt\",\"Scheduled On\":\"schedule_dt\",\n",
    "    \"Sent At\":\"schedule_dt\",\"Created\":\"schedule_dt\",\"Execution Date\":\"schedule_dt\",\n",
    "\n",
    "    # ids\n",
    "    \"Wine ID\":\"id\",\"wine_id\":\"id\",\"ID\":\"id\",\"Sku\":\"id\",\"SKU\":\"id\",\"SKU Code\":\"id\",\n",
    "    \"Item Code\":\"id\",\"Product ID\":\"id\",\"Code\":\"id\",\n",
    "\n",
    "    # names\n",
    "    \"Wine\":\"wine\",\"Name\":\"wine\",\"Product Name\":\"wine\",\"Product\":\"wine\",\n",
    "    \"Product Title\":\"wine\",\"Title\":\"wine\",\"Label\":\"wine\",\"Long Name\":\"wine\",\"Display Name\":\"wine\",\n",
    "\n",
    "    # vintage\n",
    "    \"Vintage\":\"vintage\",\"Year\":\"vintage\",\n",
    "\n",
    "    # type/colour\n",
    "    \"Full Type\":\"full_type\",\"Type\":\"full_type\",\"Colour\":\"full_type\",\"Color\":\"full_type\",\n",
    "\n",
    "    # region\n",
    "    \"Region Group\":\"region_group\",\"Region\":\"region_group\",\"Origin\":\"region_group\",\n",
    "\n",
    "    # tier/price\n",
    "    \"Price Tier\":\"price_tier\",\"Tier\":\"price_tier\",\"Price\":\"price\",\n",
    "}\n",
    "\n",
    "def _first(*vals, default=\"\"):\n",
    "    for v in vals:\n",
    "        s = str(v).strip() if v is not None else \"\"\n",
    "        if s:\n",
    "            return s\n",
    "    return default\n",
    "\n",
    "def _canon_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    orig = {c: df[c] for c in df.columns}\n",
    "\n",
    "    # rename using aliases\n",
    "    ren = {}\n",
    "    for c in df.columns:\n",
    "        if c in ALIASES:\n",
    "            ren[c] = ALIASES[c]\n",
    "    out = df.rename(columns=ren).copy()\n",
    "\n",
    "    # schedule_dt: robust parse with EU formats; auto-detect if missing\n",
    "    raw_dt = out.get(\"schedule_dt\", None)\n",
    "    parsed = None\n",
    "    if raw_dt is not None:\n",
    "        parsed = pd.to_datetime(raw_dt, errors=\"coerce\", dayfirst=True, utc=False)\n",
    "\n",
    "    if parsed is None or parsed.notna().sum() == 0:\n",
    "        for c in out.columns:\n",
    "            if re.search(r\"(schedule|send|offer|campaign|date|time)\", c, flags=re.I):\n",
    "                try:\n",
    "                    cand = pd.to_datetime(out[c], errors=\"coerce\", dayfirst=True, utc=False)\n",
    "                    if cand.notna().any():\n",
    "                        parsed = cand\n",
    "                        break\n",
    "                except Exception:\n",
    "                    pass\n",
    "    out[\"schedule_dt\"] = parsed if parsed is not None else pd.NaT\n",
    "\n",
    "    # enforce strings\n",
    "    for name, default in [(\"id\",\"\"),(\"wine\",\"\"),(\"vintage\",\"NV\"),\n",
    "                          (\"full_type\",\"\"),(\"region_group\",\"\"),(\"price_tier\",\"\")]:\n",
    "        if name not in out.columns:\n",
    "            out[name] = default\n",
    "        out[name] = out[name].astype(\"string\").fillna(\"\").str.strip()\n",
    "\n",
    "    # fix id like \"123.0\"\n",
    "    out[\"id\"] = out[\"id\"].str.replace(r\"\\.0$\", \"\", regex=True).replace({\"nan\": \"\"})\n",
    "    # vintage blanks → NV\n",
    "    out[\"vintage\"] = out[\"vintage\"].replace({\"\": \"NV\", \"nan\": \"NV\"})\n",
    "\n",
    "    # backfill wine if blank from any plausible original column\n",
    "    if out[\"wine\"].eq(\"\").any():\n",
    "        candidates = [\"Wine\",\"Name\",\"Product Name\",\"Product\",\"Product Title\",\"Title\",\"Label\",\"Long Name\",\"Display Name\"]\n",
    "        for c in candidates:\n",
    "            if c in orig:\n",
    "                mask = out[\"wine\"].eq(\"\") & orig[c].astype(str).fillna(\"\").str.strip().ne(\"\")\n",
    "                out.loc[mask, \"wine\"] = orig[c].astype(str).str.strip()\n",
    "\n",
    "    # derive price_tier from price if needed\n",
    "    if \"price\" in out.columns and out[\"price_tier\"].eq(\"\").any():\n",
    "        price_num = pd.to_numeric(out[\"price\"], errors=\"coerce\")\n",
    "        def price_bucket(x):\n",
    "            if pd.isna(x): return \"\"\n",
    "            if x < 50:   return \"Budget\"\n",
    "            if x < 100:  return \"Mid-range\"\n",
    "            if x < 200:  return \"Premium\"\n",
    "            if x < 500:  return \"Luxury\"\n",
    "            return \"Ultra Luxury\"\n",
    "        mask = out[\"price_tier\"].eq(\"\")\n",
    "        out.loc[mask, \"price_tier\"] = price_num.map(price_bucket)\n",
    "\n",
    "    # drop rows without a valid schedule_dt\n",
    "    out = out[~out[\"schedule_dt\"].isna()].copy()\n",
    "\n",
    "    # ISO components\n",
    "    iso = out[\"schedule_dt\"].dt.isocalendar()\n",
    "    out[\"iso_year\"] = iso.year.astype(int)\n",
    "    out[\"iso_week\"] = iso.week.astype(int)\n",
    "    out[\"iso_wday\"] = iso.day.astype(int)\n",
    "    return out\n",
    "\n",
    "# -------------------------------------\n",
    "# 4) Load files & combine into history\n",
    "# -------------------------------------\n",
    "frames = []\n",
    "for p in OMT_FILES:\n",
    "    df = _load_omt_file(p)\n",
    "    if not df.empty:\n",
    "        frames.append(_canon_cols(df))\n",
    "\n",
    "history_df = (\n",
    "    pd.concat(frames, ignore_index=True)\n",
    "    if frames else\n",
    "    pd.DataFrame(columns=[\"schedule_dt\",\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"price_tier\",\"iso_year\",\"iso_week\",\"iso_wday\"])\n",
    ")\n",
    "\n",
    "if history_df.empty:\n",
    "    print(\"ℹ️ No OMT history files discovered or parsable; skipping history build.\")\n",
    "    OMT_HISTORY_MAP = {}\n",
    "else:\n",
    "    # 5) Build last-campaign map (by id, with fallback to wine::vintage)\n",
    "    # max schedule_dt per id\n",
    "    last_seen_by_id = (\n",
    "        history_df.sort_values(\"schedule_dt\")\n",
    "        .groupby(\"id\", dropna=False)[\"schedule_dt\"]\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # fallback key if id is blank\n",
    "    history_df[\"name_key\"] = history_df[\"wine\"].str.strip() + \"::\" + history_df[\"vintage\"].str.strip()\n",
    "    last_seen_by_name = (\n",
    "        history_df.sort_values(\"schedule_dt\")\n",
    "        .groupby(\"name_key\")[\"schedule_dt\"]\n",
    "        .max()\n",
    "    )\n",
    "\n",
    "    # Compose history map keyed by id if present else name_key\n",
    "    OMT_HISTORY_MAP = {}\n",
    "    for _, row in history_df.iterrows():\n",
    "        key = (row[\"id\"].strip() or row[\"name_key\"])\n",
    "        if key not in OMT_HISTORY_MAP:\n",
    "            OMT_HISTORY_MAP[key] = {\"wine\": row[\"wine\"], \"vintage\": row[\"vintage\"], \"dates\": []}\n",
    "\n",
    "    # attach last_campaign_date\n",
    "    for k in list(OMT_HISTORY_MAP.keys()):\n",
    "        if k in last_seen_by_id.index and pd.notna(last_seen_by_id.loc[k]):\n",
    "            dt = last_seen_by_id.loc[k]\n",
    "        else:\n",
    "            dt = last_seen_by_name.loc[k] if k in last_seen_by_name.index else None\n",
    "        try:\n",
    "            if pd.notna(dt):\n",
    "                OMT_HISTORY_MAP[k][\"last_campaign_date\"] = pd.to_datetime(dt).date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # persist history map\n",
    "    HISTORY_JSON.write_text(json.dumps(OMT_HISTORY_MAP, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"✅ History map written → {HISTORY_JSON.name} (keys: {len(OMT_HISTORY_MAP)})\")\n",
    "\n",
    "    # 6) campaign_index.json for fast merges (exporter cell consumes this)\n",
    "    campaign_index = {\"by_id\": {}, \"by_name_vintage\": {}}\n",
    "    for k, rec in OMT_HISTORY_MAP.items():\n",
    "        last_dt = rec.get(\"last_campaign_date\")\n",
    "        if not last_dt:\n",
    "            continue\n",
    "        if \"::\" in k:\n",
    "            campaign_index[\"by_name_vintage\"][k] = {\"last_campaign_date\": last_dt}\n",
    "        else:\n",
    "            campaign_index[\"by_id\"][k] = {\"last_campaign_date\": last_dt}\n",
    "    (OUTPUT_PATH / \"campaign_index.json\").write_text(json.dumps(campaign_index, indent=2), encoding=\"utf-8\")\n",
    "    print(\"🧭 campaign_index.json written (by_id / by_name_vintage).\")\n",
    "\n",
    "    # 7) Build week calendars across ALL years in the history (so previous weeks populate correctly)\n",
    "    def _build_week_calendar(sub_df: pd.DataFrame) -> dict:\n",
    "        cal = {d: [] for d in DAYS_FULL}\n",
    "        seen_keys = set()\n",
    "        for r in sub_df.sort_values(\"schedule_dt\").to_dict(\"records\"):\n",
    "            dname = DAYMAP.get(int(r.get(\"iso_wday\", 1)), \"Monday\")\n",
    "            wine_key = (str(r.get(\"id\") or \"\").strip()\n",
    "                        or f\"{(r.get('wine') or '').strip()}::{(r.get('vintage') or 'NV').strip()}\")\n",
    "            item = {\n",
    "                \"id\": str(r.get(\"id\",\"\") or \"\"),\n",
    "                \"wine\": r.get(\"wine\") or \"Unknown\",\n",
    "                \"vintage\": (r.get(\"vintage\") or \"NV\"),\n",
    "                \"full_type\": r.get(\"full_type\") or \"Unknown\",\n",
    "                \"region_group\": r.get(\"region_group\") or \"Unknown\",\n",
    "                \"price_tier\": r.get(\"price_tier\") or \"\",\n",
    "                \"stock\": None,\n",
    "                \"match_quality\": \"History (Locked)\",\n",
    "                \"avg_cpi_score\": 0,\n",
    "                \"locked\": True,\n",
    "                \"last_campaign_date\": pd.to_datetime(r[\"schedule_dt\"]).date().isoformat(),\n",
    "            }\n",
    "            if len(cal[dname]) < NUM_SLOTS and wine_key not in seen_keys:\n",
    "                cal[dname].append(item); seen_keys.add(wine_key)\n",
    "        return cal\n",
    "    \n",
    "    # Build nested {year: {week: calendar}}\n",
    "    weeks_by_year = {}\n",
    "    for (yr, wk), sub in history_df.groupby([\"iso_year\", \"iso_week\"]):\n",
    "        weeks_by_year.setdefault(int(yr), {})[int(wk)] = _build_week_calendar(sub)\n",
    "    \n",
    "    # 8) Backfill window: ensure we have files for [current week - BACKFILL_WEEKS ... current week]\n",
    "    BACKFILL_WEEKS = int(os.getenv(\"BACKFILL_WEEKS\", \"1\"))  # includes the previous week by default\n",
    "    def _iso_prev(yr, wk):\n",
    "        # previous ISO week (handling year boundary)\n",
    "        try:\n",
    "            d = date.fromisocalendar(yr, wk, 1) - timedelta(days=7)\n",
    "            iso = d.isocalendar()\n",
    "            return iso.year, iso.week\n",
    "        except Exception:\n",
    "            return yr, max(1, wk-1)\n",
    "    \n",
    "    need_pairs = set()\n",
    "    cy, cw = calendar_year, week_number\n",
    "    y, w = cy, cw\n",
    "    for i in range(BACKFILL_WEEKS + 1):  # +1 to include current week\n",
    "        need_pairs.add((y, w))\n",
    "        y, w = _iso_prev(y, w)\n",
    "    \n",
    "    # 9) Persist per-week artifacts + index (year-aware + legacy)\n",
    "    def _atomic_write(path: Path, text: str):\n",
    "        tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "        tmp.write_text(text, encoding=\"utf-8\")\n",
    "        os.replace(tmp, path)\n",
    "    \n",
    "    index_path = OUTPUT_PATH / \"schedule_index.json\"\n",
    "    try:\n",
    "        idx = json.loads(index_path.read_text(encoding=\"utf-8\")) if index_path.exists() else {}\n",
    "    except Exception:\n",
    "        idx = {}\n",
    "    \n",
    "    latest_year = None\n",
    "    latest_week = None\n",
    "    \n",
    "    for yr, wkmap in weeks_by_year.items():\n",
    "        for wk, cal_flat in wkmap.items():\n",
    "            # If you want to persist only the backfill window, uncomment next line:\n",
    "            # if (yr, wk) not in need_pairs: continue\n",
    "    \n",
    "            # a) Year+week primary\n",
    "            out_json_year = OUTPUT_PATH / f\"weekly_campaign_schedule_{yr}_week_{wk}.json\"\n",
    "            _atomic_write(out_json_year, json.dumps(cal_flat, indent=2))\n",
    "    \n",
    "            # b) Legacy (same content) for compatibility\n",
    "            out_json_legacy = OUTPUT_PATH / f\"weekly_campaign_schedule_week_{wk}.json\"\n",
    "            _atomic_write(out_json_legacy, json.dumps(cal_flat, indent=2))\n",
    "    \n",
    "            # c) Locked snapshot (arrays of length NUM_SLOTS)\n",
    "            locked_week = {\n",
    "                day: [(cal_flat[day][i] if i < len(cal_flat[day]) else None) for i in range(NUM_SLOTS)]\n",
    "                for day in DAYS_FULL\n",
    "            }\n",
    "            out_locked_year = LOCKED_PATH / f\"locked_calendar_{yr}_week_{wk}.json\"\n",
    "            _atomic_write(out_locked_year, json.dumps(locked_week, indent=2))\n",
    "    \n",
    "            # d) Leads (empty buckets)\n",
    "            leads_json = OUTPUT_PATH / f\"weekly_leads_{yr}_week_{wk}.json\"\n",
    "            _atomic_write(leads_json, json.dumps({\"Tuesday-Thursday\": [], \"Thursday-Friday\": []}, indent=2))\n",
    "    \n",
    "            # e) Index (nested by year→week)\n",
    "            idx.setdefault(str(yr), {})[str(wk)] = {\n",
    "                \"json\": out_json_year.name,\n",
    "                \"updated_at\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "            }\n",
    "            if (latest_year is None) or (yr > latest_year) or (yr == latest_year and wk > (latest_week or 0)):\n",
    "                latest_year, latest_week = yr, wk\n",
    "    \n",
    "    # Convenience pointers\n",
    "    if latest_year is not None and latest_week is not None:\n",
    "        idx[\"_latest_year\"] = str(latest_year)\n",
    "        idx[\"_latest_week\"] = str(latest_week)\n",
    "    \n",
    "    _atomic_write(index_path, json.dumps(idx, indent=2))\n",
    "    print(f\"📅 Wrote {sum(len(w) for w in weeks_by_year.values())} week files across {len(weeks_by_year)} year(s).\")\n",
    "    print(f\"🔒 Locked calendars → {LOCKED_PATH.name}\")\n",
    "    print(\"🧹 Leads files created empty so leads boxes start blank each session.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed343c89-8c60-4258-88c7-394df9e86fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:53:27.643729Z",
     "iopub.status.busy": "2025-09-05T06:53:27.643729Z",
     "iopub.status.idle": "2025-09-05T06:53:27.658889Z",
     "shell.execute_reply": "2025-09-05T06:53:27.657879Z"
    },
    "papermill": {
     "duration": 0.019029,
     "end_time": "2025-09-05T06:53:27.659890",
     "exception": false,
     "start_time": "2025-09-05T06:53:27.640861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Week 36 (Sep 01 – Sep 07) | Season: Autumn\n",
      "🗂  Occasion: 'Notreceived'  → Folder: 2025_W36_Autumn_Notreceived\n",
      "✅ Report subfolder ready at: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\non_recipient_reports\\2025_W36_Autumn_Notreceived\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 3: Set Occasion/Subfolder Path ---\n",
    "# Creates a deterministic subfolder for this run inside REPORTS_PATH.\n",
    "# Safe with/without papermill, and robust if OCCASION is empty.\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Fallback for _iso_week_now_europe if Cell 1 wasn't executed\n",
    "try:\n",
    "    _iso_week_now_europe  # type: ignore\n",
    "except NameError:\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo\n",
    "    except Exception:\n",
    "        ZoneInfo = None  # type: ignore\n",
    "    def _iso_week_now_europe():\n",
    "        now = datetime.now(ZoneInfo(\"Europe/Zurich\")) if ZoneInfo else datetime.now()\n",
    "        iso = now.isocalendar()\n",
    "        return iso.year, iso.week\n",
    "\n",
    "# Use selected week/year from Cell 1 when available\n",
    "try:\n",
    "    _wk = int(week_number)\n",
    "except Exception:\n",
    "    _wk = _iso_week_now_europe()[1]\n",
    "\n",
    "try:\n",
    "    _year = int(calendar_year)\n",
    "except Exception:\n",
    "    _year = _iso_week_now_europe()[0]\n",
    "\n",
    "def _week_bounds(iso_week: int, year: int):\n",
    "    try:\n",
    "        start = date.fromisocalendar(year, iso_week, 1)\n",
    "    except ValueError:\n",
    "        start = date.fromisocalendar(year, min(max(iso_week, 1), 52), 1)\n",
    "    end = start + timedelta(days=6)\n",
    "    return start, end\n",
    "\n",
    "# Prefer globals from Cell 1 if present\n",
    "try:\n",
    "    _week_start = week_start_date; _week_end = week_end_date\n",
    "except Exception:\n",
    "    _week_start, _week_end = _week_bounds(_wk, _year)\n",
    "\n",
    "def _season_from_week(week: int) -> str:\n",
    "    if 9 <= week <= 21:\n",
    "        return \"Spring\"\n",
    "    elif 22 <= week <= 35:\n",
    "        return \"Summer\"\n",
    "    elif 36 <= week <= 48:\n",
    "        return \"Autumn\"\n",
    "    else:\n",
    "        return \"Winter\"\n",
    "\n",
    "_season = _season_from_week(_wk)\n",
    "\n",
    "# --- Read OCCASION from globals/papermill/env; keep your default ---\n",
    "try:\n",
    "    OCCASION  # type: ignore  # if already defined above, leave it\n",
    "except NameError:\n",
    "    OCCASION = os.getenv(\"OCCASION\", \"Notreceived\")\n",
    "\n",
    "# Optional “forced_day” kept for compatibility; ignored if falsy\n",
    "try:\n",
    "    forced_day  # type: ignore\n",
    "except NameError:\n",
    "    forced_day = None\n",
    "\n",
    "def _slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"&\", \"and\").replace(\"/\", \"-\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"\"\n",
    "\n",
    "# Compose a stable, human-readable folder name\n",
    "_occ_slug = _slugify(str(OCCASION))\n",
    "_base = f\"{_year}_W{_wk:02d}_{_season}\"\n",
    "folder_name = f\"{_base}_{_occ_slug}\" if _occ_slug else (_slugify(str(forced_day)) or _base)\n",
    "\n",
    "# Ensure REPORTS_PATH exists (from Cell 1); fall back to IRON_DATA/non_recipient_reports if missing\n",
    "try:\n",
    "    REPORTS_PATH\n",
    "except NameError:\n",
    "    REPORTS_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"non_recipient_reports\"\n",
    "REPORTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_subfolder = REPORTS_PATH / folder_name\n",
    "report_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Also expose a canonical var some downstream cells may prefer\n",
    "REPORT_RUN_PATH = report_subfolder\n",
    "\n",
    "print(\n",
    "    f\"📅 Week {_wk} \"\n",
    "    f\"({_week_start.strftime('%b %d')} – {_week_end.strftime('%b %d')}) \"\n",
    "    f\"| Season: {_season}\"\n",
    ")\n",
    "print(f\"🗂  Occasion: {OCCASION!r}  → Folder: {folder_name}\")\n",
    "print(f\"✅ Report subfolder ready at: {report_subfolder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64108b9f-dceb-4070-a4f3-d9566f088a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:53:27.662284Z",
     "iopub.status.busy": "2025-09-05T06:53:27.661882Z",
     "iopub.status.idle": "2025-09-05T06:54:45.128119Z",
     "shell.execute_reply": "2025-09-05T06:54:45.128119Z"
    },
    "papermill": {
     "duration": 77.469231,
     "end_time": "2025-09-05T06:54:45.129121",
     "exception": false,
     "start_time": "2025-09-05T06:53:27.659890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Columns in Lines: ['Salesperson Code', 'Total Order No', 'Last Offer Sent Date', 'Turnover (€)', 'Sent Emails Today', 'Sent Emails Scheduled', 'Excluded', 'Unsubscribed', 'Contact Company Name', 'Contact Person Name', 'Country', 'City', 'Language Code', 'Contact Email', 'Contact Email CC', 'Email Sent', 'Send Email Error', 'Contact Phone No.', 'Contact Mobile Phone No.', 'Informal', 'Contact No.', 'Included as New Customer', 'First Purchase Date', 'Bought Wine with exceed price', 'Bought Wine in Parent Campaign(Recall)', 'Unsubsc. for Recall', 'Buy from Competitor', 'Cont. Offer Currency']\n",
      "📋 Columns in Stats: ['Period', 'Country Code', 'Customer Post Code', 'Customer City', 'Customer Net Worth', 'Spanish Customer No.', 'Spanish Customer Name', 'Wine Country', 'Wine Overall Type', 'Wine Region', 'Wine Sub-Region', 'Wine Type', 'Classification', 'Color Code', 'Producer No.', 'Wine No.', 'Wine Name', 'Producer Name', 'Item Sales Price from Price List', 'Item No.', 'Customer No.', 'Customer Name', 'Document No.', 'Proforma No.', 'Sales Date', 'Posting Date', 'Segment Code', 'Location Code', 'Currency Code', 'Current Salesperson', 'Item Description', 'Vintage', 'Bottle Size', 'Total Bottle Quantity', 'Total Bottle Quantity in Base Size (75cl)', 'Total Bottle Amount (LCY)', 'Uploaded Qty. (Base)', 'Customer Invoice No.', 'Margin', 'Contact First Name', 'Contact Surname', 'Customer Email', 'Denomination Code', 'Item Category Code', 'Item Real Stock', 'Item Stock', 'Campaign No.', 'Document Currency Factor', 'Document Canceled', 'PA', 'PV Total', 'Salesperson']\n",
      "🧮 Filtered lines count: 50\n",
      "🔍 Sample Contact Nos: ['100145', '100174', '100277', '100339', '100412']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applied 1.12x adjustment to 9 rows for client 122636.\n",
      "✅ Final shape: (1005, 52)\n",
      "📁 Internal .pkl saved to: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\filtered_clients.pkl\n",
      "👀 Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_no</th>\n",
       "      <th>spanish_customer_no</th>\n",
       "      <th>total_bottle_amount_lcy</th>\n",
       "      <th>Period</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Customer Post Code</th>\n",
       "      <th>Customer City</th>\n",
       "      <th>Customer Net Worth</th>\n",
       "      <th>Spanish Customer Name</th>\n",
       "      <th>Wine Country</th>\n",
       "      <th>...</th>\n",
       "      <th>Denomination Code</th>\n",
       "      <th>Item Category Code</th>\n",
       "      <th>Item Real Stock</th>\n",
       "      <th>Item Stock</th>\n",
       "      <th>Campaign No.</th>\n",
       "      <th>Document Currency Factor</th>\n",
       "      <th>Document Canceled</th>\n",
       "      <th>PA</th>\n",
       "      <th>PV Total</th>\n",
       "      <th>Salesperson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>101332</td>\n",
       "      <td>nan</td>\n",
       "      <td>1108.912355</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9494</td>\n",
       "      <td>Schaan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WINE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM-25-01069</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N.BOLDRINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>101332</td>\n",
       "      <td>nan</td>\n",
       "      <td>1448.834413</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9494</td>\n",
       "      <td>Schaan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WINE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM-25-00459</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N.BOLDRINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>101332</td>\n",
       "      <td>nan</td>\n",
       "      <td>2724.413939</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9494</td>\n",
       "      <td>Schaan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FR</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WINE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N.BOLDRINI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_no spanish_customer_no  total_bottle_amount_lcy  Period  \\\n",
       "1337      101332                 nan              1108.912355    2025   \n",
       "1374      101332                 nan              1448.834413    2025   \n",
       "1375      101332                 nan              2724.413939    2025   \n",
       "\n",
       "     Country Code Customer Post Code Customer City  Customer Net Worth  \\\n",
       "1337           LI               9494        Schaan                   0   \n",
       "1374           LI               9494        Schaan                   0   \n",
       "1375           LI               9494        Schaan                   0   \n",
       "\n",
       "     Spanish Customer Name Wine Country  ... Denomination Code  \\\n",
       "1337                   NaN           IT  ...               NaN   \n",
       "1374                   NaN           US  ...               NaN   \n",
       "1375                   NaN           FR  ...               NaN   \n",
       "\n",
       "     Item Category Code Item Real Stock Item Stock Campaign No.  \\\n",
       "1337               WINE               0          0  CM-25-01069   \n",
       "1374               WINE               0          0  CM-25-00459   \n",
       "1375               WINE               0          0          NaN   \n",
       "\n",
       "     Document Currency Factor Document Canceled  PA PV Total Salesperson  \n",
       "1337                        0             False   0        0  N.BOLDRINI  \n",
       "1374                        0             False   0        0  N.BOLDRINI  \n",
       "1375                        0             False   0        0  N.BOLDRINI  \n",
       "\n",
       "[3 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 4: Load & Filter Clients for Targeting (Cloud-Safe) ---\n",
    "# Loads Lines.xlsx and Power BI stats, cleans & filters, and writes a compact\n",
    "# pickle for downstream cells. Robust to Excel float IDs (e.g., \"12345.0\").\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Inputs / Outputs ----------\n",
    "lines_path = SOURCE_PATH / \"Lines.xlsx\"\n",
    "stats_path = SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"\n",
    "output_temp_path = OUTPUT_PATH / \"filtered_clients.pkl\"\n",
    "\n",
    "def _ensure_file(p: Path, label: str):\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        print(f\"❌ Missing or empty {label}: {p}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _norm_id_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize Excel-ish ID columns:\n",
    "       - cast to string, strip\n",
    "       - drop trailing '.0'\n",
    "       - collapse common null tokens to NaN\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"string\")\n",
    "    out = (\n",
    "        s.astype(str).str.strip()\n",
    "         .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "         .replace({r\"^(nan|na|null|none|\\s*)$\", np.nan}, regex=True, inplace=False)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---------- Load ----------\n",
    "ok_lines = _ensure_file(lines_path, \"Lines.xlsx\")\n",
    "ok_stats = _ensure_file(stats_path, \"Power BI Stats.xlsx\")\n",
    "\n",
    "if not (ok_lines and ok_stats):\n",
    "    # Write empty output so downstream cells can still run gracefully\n",
    "    empty = pd.DataFrame(columns=[\"customer_no\",\"spanish_customer_no\",\"total_bottle_amount_lcy\"])\n",
    "    empty.to_pickle(output_temp_path)\n",
    "    clients_df = empty.copy()\n",
    "    print(\"⚠️ One or more inputs missing. Wrote empty filtered_clients.pkl and continuing.\")\n",
    "else:\n",
    "    # Read Excel (engine chosen by pandas)\n",
    "    lines_df = pd.read_excel(lines_path)\n",
    "    stats_df = pd.read_excel(stats_path)\n",
    "\n",
    "    # Clean column names\n",
    "    lines_df.columns = lines_df.columns.str.strip()\n",
    "    stats_df.columns = stats_df.columns.str.strip()\n",
    "\n",
    "    print(\"📋 Columns in Lines:\", lines_df.columns.tolist())\n",
    "    print(\"📋 Columns in Stats:\", stats_df.columns.tolist())\n",
    "\n",
    "    # ---------- Validate & Filter (Excluded) ----------\n",
    "    if \"Excluded\" not in lines_df.columns:\n",
    "        raise KeyError(\"❌ 'Excluded' column not found in Lines.xlsx.\")\n",
    "    \n",
    "    excluded_norm = lines_df[\"Excluded\"].apply(lambda v: str(v).strip().lower() if pd.notna(v) else \"\")\n",
    "    # Treat only explicit truthy values as excluded\n",
    "    truthy_exclude = {\"true\", \"1\", \"yes\", \"y\", \"t\"}\n",
    "    keep_mask = ~excluded_norm.isin(truthy_exclude)\n",
    "    filtered_lines = lines_df.loc[keep_mask].copy()\n",
    "    print(f\"🧮 Filtered lines count: {len(filtered_lines)}\")\n",
    "    if filtered_lines.empty:\n",
    "        print(\"⚠️ No lines passed the Excluded=False filter.\")\n",
    "\n",
    "    # ---------- Extract and normalize contact numbers ----------\n",
    "    contact_col = None\n",
    "    for c in [\"Contact No.\", \"Contact No\", \"Customer No.\", \"Customer No\", \"ContactNo\", \"CustomerNumber\"]:\n",
    "        if c in filtered_lines.columns:\n",
    "            contact_col = c\n",
    "            break\n",
    "    if contact_col is None:\n",
    "        raise KeyError(\"❌ 'Contact No.' column not found in Lines.xlsx.\")\n",
    "\n",
    "    contact_nos = _norm_id_series(filtered_lines[contact_col]).dropna().unique().tolist()\n",
    "    print(\"🔍 Sample Contact Nos:\", contact_nos[:5])\n",
    "\n",
    "    # ---------- Validate required stats columns ----------\n",
    "    required_cols = [\"Customer No.\", \"Spanish Customer No.\", \"Total Bottle Amount (LCY)\"]\n",
    "    missing = [c for c in required_cols if c not in stats_df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"❌ Missing columns in Power BI Stats file: {missing}\")\n",
    "\n",
    "    # Normalize IDs and amounts\n",
    "    stats_df[\"Customer No.\"] = _norm_id_series(stats_df[\"Customer No.\"])\n",
    "    stats_df[\"Spanish Customer No.\"] = _norm_id_series(stats_df[\"Spanish Customer No.\"])\n",
    "    stats_df[\"Total Bottle Amount (LCY)\"] = pd.to_numeric(\n",
    "        stats_df[\"Total Bottle Amount (LCY)\"], errors=\"coerce\"\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    # ---------- Business rule for client 122636 (vectorized) ----------\n",
    "    # If Customer No. == '122636' AND Spanish Customer No. is one of contact_nos:\n",
    "    #   - overwrite Customer No. with Spanish Customer No.\n",
    "    #   - multiply Total Bottle Amount (LCY) by 1.12\n",
    "    mask_122636 = (stats_df[\"Customer No.\"] == \"122636\") & (\n",
    "        stats_df[\"Spanish Customer No.\"].isin(contact_nos)\n",
    "    )\n",
    "    if mask_122636.any():\n",
    "        stats_df.loc[mask_122636, \"Customer No.\"] = stats_df.loc[mask_122636, \"Spanish Customer No.\"]\n",
    "        stats_df.loc[mask_122636, \"Total Bottle Amount (LCY)\"] *= 1.12\n",
    "        print(f\"🔧 Applied 1.12x adjustment to {mask_122636.sum()} rows for client 122636.\")\n",
    "\n",
    "    # ---------- Final filter: keep only valid clients ----------\n",
    "    final_df = stats_df.loc[\n",
    "        stats_df[\"Customer No.\"].isin(contact_nos)\n",
    "    ].copy()\n",
    "\n",
    "    # ---------- Normalize output schema ----------\n",
    "    final_df = final_df.rename(\n",
    "        columns={\n",
    "            \"Customer No.\": \"customer_no\",\n",
    "            \"Spanish Customer No.\": \"spanish_customer_no\",\n",
    "            \"Total Bottle Amount (LCY)\": \"total_bottle_amount_lcy\",\n",
    "        }\n",
    "    )\n",
    "    # keep only the essentials + anything you know downstream needs\n",
    "    core_cols = [\"customer_no\", \"spanish_customer_no\", \"total_bottle_amount_lcy\"]\n",
    "    # If there are extra useful columns, keep them but put core first\n",
    "    extras = [c for c in final_df.columns if c not in core_cols]\n",
    "    final_df = final_df[core_cols + extras]\n",
    "\n",
    "    # Persist compact binary for downstream cells\n",
    "    final_df.to_pickle(output_temp_path)\n",
    "\n",
    "    # Expose for the rest of the notebook\n",
    "    clients_df = final_df\n",
    "\n",
    "    # ---------- Logs ----------\n",
    "    print(f\"✅ Final shape: {final_df.shape}\")\n",
    "    print(f\"📁 Internal .pkl saved to: {output_temp_path}\")\n",
    "    if final_df.empty:\n",
    "        print(\"⚠️ The output file contains only headers — no matching customers found.\")\n",
    "    else:\n",
    "        print(\"👀 Sample rows:\")\n",
    "        try:\n",
    "            from IPython.display import display  # safe if IPython present\n",
    "            display(final_df.head(3))\n",
    "        except Exception:\n",
    "            print(final_df.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c50a82-2316-45fd-879a-4e4a959f2d1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:54:45.131121Z",
     "iopub.status.busy": "2025-09-05T06:54:45.131121Z",
     "iopub.status.idle": "2025-09-05T06:55:12.291492Z",
     "shell.execute_reply": "2025-09-05T06:55:12.291492Z"
    },
    "papermill": {
     "duration": 27.161371,
     "end_time": "2025-09-05T06:55:12.291492",
     "exception": false,
     "start_time": "2025-09-05T06:54:45.130121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Stock columns (raw): ['ID', 'Stock', 'Minimum Order Q-ty', 'Size', 'Wine', 'Producer', 'Origin', 'Classification', 'Region', 'Country', 'Color', 'Type', 'Vintage', 'Rating', 'RP', 'WA', 'WS', 'JS', 'RG', 'VINOUS', 'JR', 'Decanter', 'NM', 'JA', 'AG', 'Falstaff', 'JD', 'DB', 'WInd', 'WCI', 'YB', 'AMA', 'JMQ', 'MDM', 'VINUM', 'AVG', 'EUR p/bt', 'CHF p/bt VAT excl.', 'CHF p/bt VAT incl.', 'OMT Last Offer Price CHF', 'OMT Last Offer Price Euro', 'OMT last offer date']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final wine dataset enriched and saved to: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\stock_df_final.pkl\n",
      "   Rows: 4,447 | With price tiers: 4,447\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stock</th>\n",
       "      <th>Minimum Order Q-ty</th>\n",
       "      <th>size</th>\n",
       "      <th>wine</th>\n",
       "      <th>producer</th>\n",
       "      <th>origin</th>\n",
       "      <th>classification</th>\n",
       "      <th>region</th>\n",
       "      <th>Country</th>\n",
       "      <th>...</th>\n",
       "      <th>Num_of_CM</th>\n",
       "      <th>region_group</th>\n",
       "      <th>type_class</th>\n",
       "      <th>bottle_size_ml</th>\n",
       "      <th>grape_list</th>\n",
       "      <th>body</th>\n",
       "      <th>sweetness</th>\n",
       "      <th>occasion</th>\n",
       "      <th>full_type</th>\n",
       "      <th>wine_full_match_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40261</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Bolaire</td>\n",
       "      <td>Château Bolaire</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Red</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Bolaire Château Bolaire 2016 75.0 40261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54770</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Ygrec de Ch. d'Yquem</td>\n",
       "      <td>Château d'Yquem</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>White</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Still White</td>\n",
       "      <td>Ygrec de Ch. d'Yquem Château d'Yquem 2020 75.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62182</td>\n",
       "      <td>108</td>\n",
       "      <td>6</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Ygrec de Ch. d'Yquem</td>\n",
       "      <td>Château d'Yquem</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>White</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Still White</td>\n",
       "      <td>Ygrec de Ch. d'Yquem Château d'Yquem 2022 75.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  stock  Minimum Order Q-ty  size                  wine  \\\n",
       "0  40261    108                  12  75.0               Bolaire   \n",
       "1  54770      3                   1  75.0  Ygrec de Ch. d'Yquem   \n",
       "2  62182    108                   6  75.0  Ygrec de Ch. d'Yquem   \n",
       "\n",
       "          producer    origin classification    region Country  ... Num_of_CM  \\\n",
       "0  Château Bolaire  Bordeaux            NaN  Bordeaux  France  ...       NaN   \n",
       "1  Château d'Yquem  Bordeaux            NaN  Bordeaux  France  ...       NaN   \n",
       "2  Château d'Yquem  Bordeaux            NaN  Bordeaux  France  ...      22.0   \n",
       "\n",
       "  region_group type_class bottle_size_ml grape_list body sweetness occasion  \\\n",
       "0     Bordeaux        Red            750    Unknown    4         3   Casual   \n",
       "1     Bordeaux      White            750    Unknown    2         3   Dinner   \n",
       "2     Bordeaux      White            750    Unknown    2         3   Dinner   \n",
       "\n",
       "     full_type                             wine_full_match_string  \n",
       "0    Still Red            Bolaire Château Bolaire 2016 75.0 40261  \n",
       "1  Still White  Ygrec de Ch. d'Yquem Château d'Yquem 2020 75.0...  \n",
       "2  Still White  Ygrec de Ch. d'Yquem Château d'Yquem 2022 75.0...  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 5: Enhance Stock Dataset (Detailed Stock + OMT Info) ---\n",
    "# Robust load, normalize, enrich with price tiers (label + stable key),\n",
    "# trait inference, and OMT campaign summary. Outputs stock_df_final.pkl.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Safe unidecode import (fallback) ---\n",
    "try:\n",
    "    from unidecode import unidecode  # pip install Unidecode\n",
    "except Exception:\n",
    "    import unicodedata\n",
    "    def unidecode(x):\n",
    "        x = '' if x is None else str(x)\n",
    "        return ''.join(\n",
    "            ch for ch in unicodedata.normalize('NFKD', x)\n",
    "            if not unicodedata.combining(ch)\n",
    "        )\n",
    "\n",
    "stock_path = SOURCE_PATH / \"Detailed Stock List.xlsx\"\n",
    "omt_path   = SOURCE_PATH / \"OMT Main Offer List.xlsx\"\n",
    "\n",
    "def _ensure_file(p: Path, label: str) -> bool:\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        print(f\"❌ Missing or empty {label}: {p}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ok_stock = _ensure_file(stock_path, \"Detailed Stock List.xlsx\")\n",
    "ok_omt   = _ensure_file(omt_path, \"OMT Main Offer List.xlsx\")\n",
    "\n",
    "if not ok_stock:\n",
    "    # Minimal empty frame so downstream cells can still run.\n",
    "    stock_df = pd.DataFrame(columns=[\n",
    "        \"id\",\"wine\",\"producer\",\"classification\",\"region\",\"type\",\"color\",\"size\",\"vintage\",\n",
    "        \"origin\",\"stock\",\"CHF Price\",\"price_tier\",\"price_tier_key\",\"full_type\",\"region_group\",\n",
    "        \"bottle_size_ml\",\"grape_list\",\"body\",\"sweetness\",\"occasion\",\"wine_full_match_string\",\n",
    "        \"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "    ])\n",
    "    final_stock_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df.to_pickle(final_stock_path)\n",
    "    print(f\"⚠️ No stock file. Wrote empty dataset to: {final_stock_path}\")\n",
    "else:\n",
    "    # ---- Load Detailed Stock (header fallback: row 2 first, then row 0) ----\n",
    "    try:\n",
    "        detailed_df = pd.read_excel(stock_path, header=2)\n",
    "        # If the expected key columns aren't present, retry with header=0\n",
    "        if not any(k in detailed_df.columns for k in [\"ID\",\"Item No.\",\"Wine\",\"Stock\"]):\n",
    "            detailed_df = pd.read_excel(stock_path, header=0)\n",
    "    except Exception:\n",
    "        detailed_df = pd.read_excel(stock_path, header=0)\n",
    "\n",
    "    # ---- Flexible rename map (accept common variants) ----\n",
    "    print(\"📋 Stock columns (raw):\", detailed_df.columns.tolist())\n",
    "    rename_map_variants = {\n",
    "        \"ID\": \"id\",\n",
    "        \"Item No.\": \"id\",\n",
    "        \"Wine\": \"wine\",\n",
    "        \"Producer\": \"producer\",\n",
    "        \"Classification\": \"classification\",\n",
    "        \"Region\": \"region\",\n",
    "        \"Type\": \"type\",\n",
    "        \"Color\": \"color\",\n",
    "        \"Size\": \"size\",\n",
    "        \"Vintage\": \"vintage\",\n",
    "        \"Origin\": \"origin\",\n",
    "        \"Stock\": \"stock\",\n",
    "        \"Qty\": \"stock\",\n",
    "        \"CHF p/bt VAT excl.\": \"CHF Price\",\n",
    "        \"CHF p/bt (VAT excl.)\": \"CHF Price\",\n",
    "        \"Unit Price (CHF)\": \"CHF Price\",\n",
    "        \"CHF Price\": \"CHF Price\",\n",
    "        \"AVG\": \"avg_score\",\n",
    "        \"Average Score\": \"avg_score\",\n",
    "    }\n",
    "    actual_map = {c: rename_map_variants[c] for c in detailed_df.columns if c in rename_map_variants}\n",
    "    detailed_df = detailed_df.rename(columns=actual_map)\n",
    "\n",
    "    # ---- Ensure core columns exist ----\n",
    "    for c in [\"id\",\"wine\",\"producer\",\"classification\",\"region\",\"type\",\"color\",\"size\",\"vintage\",\"origin\",\"stock\",\"CHF Price\",\"avg_score\"]:\n",
    "        if c not in detailed_df.columns:\n",
    "            detailed_df[c] = np.nan\n",
    "\n",
    "    # ---- Normalize critical fields ----\n",
    "    detailed_df[\"id\"] = (\n",
    "        detailed_df[\"id\"].astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .replace({\"nan\": \"\", \"None\": \"\"})\n",
    "    )\n",
    "    # CHF Price → numeric\n",
    "    detailed_df[\"CHF Price\"] = (\n",
    "        detailed_df[\"CHF Price\"]\n",
    "        .astype(str).str.replace(r\"[^\\d\\.,-]\", \"\", regex=True)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    detailed_df[\"CHF Price\"] = pd.to_numeric(detailed_df[\"CHF Price\"], errors=\"coerce\")\n",
    "    # Stock → numeric, missing → 0\n",
    "    detailed_df[\"stock\"] = pd.to_numeric(detailed_df[\"stock\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # ---- OMT Campaign Summary (robust headers & normalization) ----\n",
    "    if not ok_omt:\n",
    "        campaign_summary = pd.DataFrame(columns=[\n",
    "            \"Item No.\",\"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "        ])\n",
    "    else:\n",
    "        omt_df = pd.read_excel(omt_path)\n",
    "\n",
    "        # Canonicalize common OMT headers\n",
    "        id_aliases    = [\"Item No.\",\"Item No\",\"Item\",\"ID\",\"Wine ID\",\"Sku\",\"SKU\",\"SKU Code\",\"Item Code\",\"Product ID\",\"Code\"]\n",
    "        sched_aliases = [\"Schedule DateTime\",\"Scheduled\",\"Scheduled At\",\"DateTime\",\"Date\",\"Send Date\",\"Offer Date\",\n",
    "                         \"Campaign Date\",\"Scheduled Date\",\"Scheduled On\",\"Sent At\",\"Created\",\"Execution Date\"]\n",
    "        eur_price_aliases = [\"Unit Price (EUR)\",\"EUR Unit Price\",\"EUR Price\",\"Price (EUR)\"]\n",
    "        chf_price_aliases = [\"Unit Price (CHF)\",\"CHF Unit Price\",\"CHF Price\",\"Unit Price\"]\n",
    "        sent_aliases  = [\"Number of Sent Emails\",\"Sent Emails\",\"Emails Sent\"]\n",
    "\n",
    "        def _pick(cands, df):\n",
    "            for c in cands:\n",
    "                if c in df.columns: return c\n",
    "            return None\n",
    "\n",
    "        id_col    = _pick(id_aliases, omt_df)\n",
    "        # If no ID, we can't summarize meaningfully\n",
    "        if id_col is None:\n",
    "            campaign_summary = pd.DataFrame(columns=[\n",
    "                \"Item No.\",\"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "            ])\n",
    "        else:\n",
    "            sched_col = _pick(sched_aliases, omt_df)\n",
    "            eur_col   = _pick(eur_price_aliases, omt_df)\n",
    "            chf_col   = _pick(chf_price_aliases, omt_df)\n",
    "            sent_col  = _pick(sent_aliases, omt_df)\n",
    "\n",
    "            omt_df = omt_df.rename(columns={id_col: \"Item No.\"})\n",
    "            omt_df[\"Item No.\"] = (\n",
    "                omt_df[\"Item No.\"].astype(str).str.strip()\n",
    "                .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "            )\n",
    "\n",
    "            if sched_col:\n",
    "                omt_df[\"Schedule DateTime\"] = pd.to_datetime(omt_df[sched_col], errors=\"coerce\", dayfirst=True, utc=False)\n",
    "            else:\n",
    "                omt_df[\"Schedule DateTime\"] = pd.NaT\n",
    "\n",
    "            omt_df[\"Unit Price (EUR)\"] = pd.to_numeric(omt_df[eur_col], errors=\"coerce\") if eur_col else np.nan\n",
    "            omt_df[\"Unit Price\"]       = pd.to_numeric(omt_df[chf_col], errors=\"coerce\") if chf_col else np.nan\n",
    "            omt_df[\"Number of Sent Emails\"] = pd.to_numeric(omt_df[sent_col], errors=\"coerce\") if sent_col else np.nan\n",
    "\n",
    "            # Sort so “last” really is most recent in each group\n",
    "            omt_df = omt_df.sort_values([\"Item No.\",\"Schedule DateTime\"], ascending=[True, True])\n",
    "\n",
    "            counts = omt_df.groupby(\"Item No.\").size().rename(\"Num_of_CM\")\n",
    "            last_rows = omt_df.groupby(\"Item No.\").tail(1)[\n",
    "                [\"Item No.\",\"Schedule DateTime\",\"Unit Price (EUR)\",\"Unit Price\",\"Number of Sent Emails\"]\n",
    "            ]\n",
    "\n",
    "            campaign_summary = (\n",
    "                last_rows.rename(columns={\n",
    "                    \"Schedule DateTime\": \"most_recent_date\",\n",
    "                    \"Unit Price (EUR)\": \"last_eur_price\",\n",
    "                    \"Unit Price\": \"last_chf_price\",\n",
    "                    \"Number of Sent Emails\": \"number_of_sent_emails\",\n",
    "                })\n",
    "                .merge(counts, left_on=\"Item No.\", right_index=True, how=\"left\")\n",
    "            )\n",
    "\n",
    "    # ---- Price tier logic (label + stable key) ----\n",
    "    def price_tier_label(price):\n",
    "        try:\n",
    "            p = float(price)\n",
    "            if p < 50:   return \"Budget\"\n",
    "            if p < 100:  return \"Mid-range\"\n",
    "            if p < 200:  return \"Premium\"\n",
    "            if p < 500:  return \"Luxury\"\n",
    "            return \"Ultra Luxury\"\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def price_tier_key_from_label(label):\n",
    "        m = (label or \"\").strip().lower()\n",
    "        return {\n",
    "            \"budget\": \"budget\",\n",
    "            \"mid-range\": \"mid\",\n",
    "            \"premium\": \"premium\",\n",
    "            \"luxury\": \"luxury\",\n",
    "            \"ultra luxury\": \"ultra\",\n",
    "        }.get(m, \"\")\n",
    "\n",
    "    detailed_df[\"price_tier\"] = detailed_df[\"CHF Price\"].apply(price_tier_label)\n",
    "    detailed_df[\"price_tier_key\"] = detailed_df[\"price_tier\"].apply(price_tier_key_from_label)\n",
    "\n",
    "    # ---- Merge OMT summary → stock ----\n",
    "    enhanced_df = detailed_df.merge(\n",
    "        campaign_summary, how=\"left\", left_on=\"id\", right_on=\"Item No.\"\n",
    "    )\n",
    "\n",
    "    # Region group fallback (handle empty strings)\n",
    "    enhanced_df[\"region_group\"] = (\n",
    "        enhanced_df[\"origin\"].replace(\"\", np.nan)\n",
    "        .fillna(enhanced_df[\"region\"])\n",
    "    )\n",
    "    enhanced_df[\"most_recent_date\"] = pd.to_datetime(enhanced_df[\"most_recent_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ---- Helpers / inference ----\n",
    "    def infer_type_class(row):\n",
    "        t = unidecode(str(row.get(\"type\", \"\")).lower())\n",
    "        c = unidecode(str(row.get(\"color\", \"\")).lower())\n",
    "        txt = f\"{t} {c}\"\n",
    "        if \"sparkling\" in txt or \"champagne\" in txt or \"cava\" in txt or \"prosecco\" in txt:\n",
    "            return \"Sparkling\"\n",
    "        if \"dessert\" in txt or \"sweet\" in txt or \"sauternes\" in txt or \"porto\" in txt:\n",
    "            return \"Dessert\"\n",
    "        if \"rose\" in txt or \"rosé\" in txt:\n",
    "            return \"Rose\"\n",
    "        if \"white\" in txt or \"blanc\" in txt or \"bianco\" in txt:\n",
    "            return \"White\"\n",
    "        if \"red\" in txt or \"rouge\" in txt or \"rosso\" in txt:\n",
    "            return \"Red\"\n",
    "        # Default to wine \"type\" signal if available\n",
    "        return \"Red\" if \"red\" in t else (\"White\" if \"white\" in t else \"Red\")\n",
    "\n",
    "    def parse_bottle_size_ml(val):\n",
    "        s = unidecode(str(val)).lower().strip()\n",
    "        if not s or s == \"nan\": return np.nan\n",
    "        # named formats\n",
    "        if \"jeroboam\" in s: return 3000\n",
    "        if \"magnum\" in s:   return 1500\n",
    "        # normalize units\n",
    "        s = s.replace(\"lt\", \"l\")\n",
    "        # litres forms like \"0.75 l\"\n",
    "        if \"l\" in s and \"ml\" not in s and \"cl\" not in s:\n",
    "            num = re.sub(r\"[^\\d\\.]\", \"\", s)\n",
    "            try: return int(float(num) * 1000)  # L → ml\n",
    "            except Exception: return np.nan\n",
    "        # numeric like \"75cl\" or \"750ml\"\n",
    "        s = s.replace(\"cl\",\"\").replace(\"ml\",\"\").strip()\n",
    "        s = re.sub(r\"[^\\d\\.]\", \"\", s)\n",
    "        try:\n",
    "            num = float(s)\n",
    "            # if < 100, assume cl → convert to ml\n",
    "            return int(num * 10) if num < 100 else int(num)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def infer_grapes(row):\n",
    "        wine_name = unidecode(str(row.get('wine', '')).lower().strip())\n",
    "        producer  = unidecode(str(row.get('producer', '')).lower().strip())\n",
    "        origin    = unidecode(str(row.get('origin', '')).lower().strip())\n",
    "        combined  = f\"{wine_name} {producer} {origin}\"\n",
    "\n",
    "        grape_keywords = {\n",
    "            'nebbiolo':'Nebbiolo','tempranillo':'Tempranillo','cabernet sauvignon':'Cabernet Sauvignon',\n",
    "            ' cabernet ':'Cabernet Sauvignon',' cab ':'Cabernet Sauvignon','merlot':'Merlot',\n",
    "            'pinot noir':'Pinot Noir',' pinot ':'Pinot Noir','sangiovese':'Sangiovese','syrah':'Syrah',\n",
    "            'shiraz':'Syrah','grenache':'Grenache','chardonnay':'Chardonnay',' chard ':'Chardonnay',\n",
    "            'riesling':'Riesling','sauvignon blanc':'Sauvignon Blanc',' sauvignon ':'Sauvignon Blanc',\n",
    "            ' sauv ':'Sauvignon Blanc','zinfandel':'Zinfandel','primitivo':'Primitivo','malbec':'Malbec',\n",
    "            'grigio':'Pinot Grigio','garganega':'Garganega',\"nero d avola\":\"Nero d'Avola\",'barbera':'Barbera',\n",
    "            'carmenere':'Carmenère','trebbiano':'Trebbiano','vermentino':'Vermentino','teroldego':'Teroldego'\n",
    "        }\n",
    "        hits = {g for k,g in grape_keywords.items() if k in f\" {combined} \"}\n",
    "        if not hits:\n",
    "            t = unidecode(str(row.get('type',''))).lower()\n",
    "            if 'red' in t: hits.add('Red Blend')\n",
    "            elif 'white' in t: hits.add('White Blend')\n",
    "            elif 'sparkling' in t: hits.add('Sparkling Blend')\n",
    "            elif 'rose' in t or 'rosé' in t: hits.add('Rosé Blend')\n",
    "            else: hits.add('Unknown')\n",
    "        return '/'.join(sorted(hits))\n",
    "\n",
    "    def infer_body(row):\n",
    "        c = str(row.get(\"color\",\"\")).lower()\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if \"sparkling\" in t: return 2\n",
    "        if \"red\" in c: return 4\n",
    "        if \"white\" in c or \"ros\" in c: return 2\n",
    "        return 3\n",
    "\n",
    "    def infer_sweetness(row):\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if \"brut\" in t or \"dry\" in t: return 1\n",
    "        if \"sweet\" in t: return 5\n",
    "        if \"medium\" in t: return 3\n",
    "        return 3\n",
    "\n",
    "    def infer_occasion(row):\n",
    "        tier = row.get(\"price_tier\")\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if tier in [\"Luxury\",\"Ultra Luxury\"]: return \"Gifting\"\n",
    "        if \"sparkling\" in t or \"ros\" in t:     return \"Celebration\"\n",
    "        if tier in [\"Mid-range\",\"Premium\"]:    return \"Dinner\"\n",
    "        return \"Casual\"\n",
    "\n",
    "    # Ensure text cols exist to avoid \"nan\" literals\n",
    "    for tcol in [\"wine\",\"producer\",\"region\",\"type\",\"color\",\"size\",\"vintage\"]:\n",
    "        enhanced_df[tcol] = enhanced_df[tcol].fillna(\"\").astype(str)\n",
    "\n",
    "    enhanced_df[\"type_class\"]     = enhanced_df.apply(infer_type_class, axis=1)\n",
    "    enhanced_df[\"bottle_size_ml\"] = enhanced_df[\"size\"].apply(parse_bottle_size_ml)\n",
    "    enhanced_df[\"grape_list\"]     = enhanced_df.apply(infer_grapes, axis=1)\n",
    "    enhanced_df[\"body\"]           = enhanced_df.apply(infer_body, axis=1)\n",
    "    enhanced_df[\"sweetness\"]      = enhanced_df.apply(infer_sweetness, axis=1)\n",
    "    enhanced_df[\"occasion\"]       = enhanced_df.apply(infer_occasion, axis=1)\n",
    "\n",
    "    # --- full_type: vectorized ---\n",
    "    t = enhanced_df[\"type\"].fillna(\"\").astype(str).str.strip()\n",
    "    c = enhanced_df[\"color\"].fillna(\"\").astype(str).str.strip()\n",
    "    enhanced_df[\"full_type\"] = (t.str.title() + (\" \" + c.str.title()).where(c.ne(\"\"), \"\")).str.strip()\n",
    "\n",
    "    # Stable dedupe key for scheduling logic if needed\n",
    "    enhanced_df[\"wine_full_match_string\"] = (\n",
    "        enhanced_df[\"wine\"].str.strip() + \" \" +\n",
    "        enhanced_df[\"producer\"].str.strip() + \" \" +\n",
    "        enhanced_df[\"vintage\"].astype(str).str.strip() + \" \" +\n",
    "        enhanced_df[\"size\"].astype(str).str.strip() + \" \" +\n",
    "        enhanced_df[\"id\"].astype(str).str.strip()\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    # Final save\n",
    "    final_stock_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    enhanced_df.to_pickle(final_stock_path)\n",
    "\n",
    "    stock_df = enhanced_df  # expose to later cells\n",
    "\n",
    "    print(f\"✅ Final wine dataset enriched and saved to: {final_stock_path}\")\n",
    "    print(f\"   Rows: {len(enhanced_df):,} | With price tiers: {enhanced_df['price_tier'].notna().sum():,}\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(enhanced_df.head(3))\n",
    "    except Exception:\n",
    "        print(enhanced_df.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c13cb8-fd29-448e-b526-2380df9c71af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:12.291492Z",
     "iopub.status.busy": "2025-09-05T06:55:12.291492Z",
     "iopub.status.idle": "2025-09-05T06:55:27.652473Z",
     "shell.execute_reply": "2025-09-05T06:55:27.652473Z"
    },
    "papermill": {
     "duration": 15.362985,
     "end_time": "2025-09-05T06:55:27.654477",
     "exception": false,
     "start_time": "2025-09-05T06:55:12.291492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍾 Bottle size [750ml] → wines: 4447 → 2342\n",
      "⚠️ Last stock < 10 → wines: 2342 → 756\n",
      "⏮️ Recency filter → removed 25 (source: weekly_campaign_schedule_2025_week_36.pkl)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   0%|                                            | 0/1219 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   1%|▎                                 | 12/1219 [00:00<00:10, 115.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   2%|▋                                 | 24/1219 [00:00<00:11, 101.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   3%|▉                                 | 35/1219 [00:00<00:11, 100.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   4%|█▎                                 | 46/1219 [00:00<00:11, 98.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   5%|█▌                                 | 56/1219 [00:00<00:11, 98.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   6%|█▉                                | 68/1219 [00:00<00:11, 102.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   7%|██▎                               | 82/1219 [00:00<00:10, 112.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   8%|██▋                               | 98/1219 [00:00<00:09, 123.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:   9%|███                              | 111/1219 [00:01<00:09, 116.51it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  10%|███▎                             | 123/1219 [00:01<00:10, 108.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  11%|███▋                              | 134/1219 [00:01<00:11, 96.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  12%|████                              | 144/1219 [00:01<00:11, 96.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  13%|████▏                            | 155/1219 [00:01<00:10, 100.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  14%|████▍                            | 166/1219 [00:01<00:10, 100.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  15%|████▉                            | 181/1219 [00:01<00:09, 111.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  16%|█████▎                           | 194/1219 [00:01<00:08, 116.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  17%|█████▋                           | 209/1219 [00:01<00:08, 124.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  18%|██████                           | 222/1219 [00:02<00:09, 103.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  20%|██████▍                          | 239/1219 [00:02<00:08, 114.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  21%|██████▊                          | 253/1219 [00:02<00:08, 119.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  22%|███████▏                         | 267/1219 [00:02<00:07, 124.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  23%|███████▋                         | 282/1219 [00:02<00:07, 129.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  24%|████████                         | 296/1219 [00:02<00:07, 131.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  26%|████████▍                        | 312/1219 [00:02<00:06, 135.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  27%|████████▊                        | 326/1219 [00:02<00:06, 133.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  28%|█████████▎                       | 344/1219 [00:02<00:06, 141.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  29%|█████████▋                       | 359/1219 [00:03<00:06, 139.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  31%|██████████                       | 373/1219 [00:03<00:06, 136.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  32%|██████████▍                      | 387/1219 [00:03<00:06, 134.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  33%|██████████▉                      | 403/1219 [00:03<00:05, 140.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  34%|███████████▎                     | 419/1219 [00:03<00:05, 145.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  36%|███████████▋                     | 434/1219 [00:03<00:05, 146.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  37%|████████████▏                    | 451/1219 [00:03<00:05, 152.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  38%|████████████▋                    | 468/1219 [00:03<00:04, 155.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  40%|█████████████▏                   | 485/1219 [00:03<00:04, 153.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  41%|█████████████▌                   | 501/1219 [00:04<00:04, 152.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  42%|█████████████▉                   | 517/1219 [00:04<00:04, 151.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  44%|██████████████▍                  | 533/1219 [00:04<00:04, 138.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  45%|██████████████▊                  | 549/1219 [00:04<00:04, 139.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  46%|███████████████▎                 | 564/1219 [00:04<00:04, 135.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  47%|███████████████▋                 | 578/1219 [00:04<00:05, 115.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  49%|████████████████                 | 593/1219 [00:04<00:05, 122.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  50%|████████████████▍                | 609/1219 [00:04<00:04, 130.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  51%|████████████████▉                | 624/1219 [00:04<00:04, 134.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  52%|█████████████████▎               | 639/1219 [00:05<00:04, 132.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  54%|█████████████████▋               | 654/1219 [00:05<00:04, 136.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  55%|██████████████████               | 669/1219 [00:05<00:03, 138.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  56%|██████████████████▌              | 684/1219 [00:05<00:03, 140.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  58%|██████████████████▉              | 701/1219 [00:05<00:03, 147.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  59%|███████████████████▍             | 720/1219 [00:05<00:03, 150.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  60%|███████████████████▉             | 736/1219 [00:05<00:03, 152.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  62%|████████████████████▍            | 753/1219 [00:05<00:03, 152.23it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  63%|████████████████████▊            | 769/1219 [00:05<00:02, 150.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  64%|█████████████████████▎           | 785/1219 [00:06<00:02, 151.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  66%|█████████████████████▋           | 801/1219 [00:06<00:02, 147.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  67%|██████████████████████           | 816/1219 [00:06<00:02, 145.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  68%|██████████████████████▍          | 831/1219 [00:06<00:02, 143.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  69%|██████████████████████▉          | 846/1219 [00:06<00:02, 144.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  71%|███████████████████████▎         | 862/1219 [00:06<00:02, 147.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  72%|███████████████████████▋         | 877/1219 [00:06<00:02, 147.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  73%|████████████████████████▏        | 892/1219 [00:06<00:02, 145.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  74%|████████████████████████▌        | 908/1219 [00:06<00:02, 149.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  76%|████████████████████████▉        | 923/1219 [00:07<00:02, 146.48it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  77%|█████████████████████████▍       | 938/1219 [00:07<00:01, 143.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  78%|█████████████████████████▊       | 953/1219 [00:07<00:02, 117.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  79%|██████████████████████████▏      | 966/1219 [00:07<00:02, 120.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  80%|██████████████████████████▌      | 980/1219 [00:07<00:01, 125.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  82%|██████████████████████████▉      | 994/1219 [00:07<00:01, 127.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  83%|██████████████████████████▍     | 1009/1219 [00:07<00:01, 133.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  84%|██████████████████████████▉     | 1025/1219 [00:07<00:01, 140.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  85%|███████████████████████████▎    | 1040/1219 [00:07<00:01, 142.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  87%|███████████████████████████▋    | 1055/1219 [00:08<00:01, 139.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  88%|████████████████████████████    | 1070/1219 [00:08<00:01, 141.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  89%|████████████████████████████▌   | 1086/1219 [00:08<00:00, 140.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  90%|████████████████████████████▉   | 1101/1219 [00:08<00:00, 142.93it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  92%|█████████████████████████████▎  | 1118/1219 [00:08<00:00, 147.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  93%|█████████████████████████████▋  | 1133/1219 [00:08<00:00, 148.21it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  94%|██████████████████████████████▏ | 1148/1219 [00:08<00:00, 147.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  95%|██████████████████████████████▌ | 1163/1219 [00:08<00:00, 145.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  97%|██████████████████████████████▉ | 1178/1219 [00:08<00:00, 138.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  98%|███████████████████████████████▎| 1193/1219 [00:08<00:00, 139.53it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors:  99%|███████████████████████████████▋| 1209/1219 [00:09<00:00, 140.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "🔄 Generating CPI vectors: 100%|████████████████████████████████| 1219/1219 [00:09<00:00, 132.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ CPI computation completed in 9.61 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preferences, CPI matrix, and UI stock snapshot saved (.pkl).\n",
      "🧪 CPI Matrix shape: (1422, 1221) | UI stock cols: ['id', 'wine', 'producer', 'vintage', 'price_tier', 'stock', 'full_type', 'type', 'color', 'region', 'region_group', 'bottle_size_ml', 'avg_score', 'high_score', 'avg_cpi_score']\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 6: CPI Calculation + Filter Wiring (Clean & Robust) ---\n",
    "# Honors UI filters (loyalty, wine type, bottle size, price tier, last stock, seasonality),\n",
    "# computes CPI against stock, and saves compact outputs the UI can reuse.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_season_from_week(week_no: int):\n",
    "    if 1 <= week_no <= 8 or 49 <= week_no <= 53: return \"Winter\"\n",
    "    if 9  <= week_no <= 22: return \"Spring\"\n",
    "    if 23 <= week_no <= 35: return \"Summer\"\n",
    "    if 36 <= week_no <= 48: return \"Autumn\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_ids_from_weekly_calendar(obj) -> set:\n",
    "    ids = set()\n",
    "    if isinstance(obj, dict):\n",
    "        for arr in obj.values():\n",
    "            if isinstance(arr, list):\n",
    "                for it in arr:\n",
    "                    if isinstance(it, dict):\n",
    "                        v = it.get(\"id\") or it.get(\"wine_id\")\n",
    "                        if v is not None and str(v).strip():\n",
    "                            ids.add(str(v))\n",
    "    elif isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            if isinstance(it, dict):\n",
    "                v = it.get(\"id\") or it.get(\"wine_id\")\n",
    "                if v is not None and str(v).strip():\n",
    "                    ids.add(str(v))\n",
    "    return ids\n",
    "\n",
    "def coerce_numeric(s, default=np.nan):\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# keep the calendar_year chosen earlier; do NOT reset to now().year later\n",
    "try:\n",
    "    week_number = int(week_number)\n",
    "except Exception:\n",
    "    week_number = int(os.getenv(\"WEEK_NUMBER\", datetime.now().isocalendar().week))\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "except Exception:\n",
    "    year = datetime.now().year\n",
    "\n",
    "selected_season = get_season_from_week(week_number)\n",
    "\n",
    "# text-mode tqdm (avoid widget deps)\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "# ---------- Ensure inputs are present ----------\n",
    "# If notebook was run standalone, try loading from previous cell outputs\n",
    "if 'clients_df' not in globals():\n",
    "    p = OUTPUT_PATH / \"filtered_clients.pkl\"\n",
    "    clients_df = pd.read_pickle(p) if p.exists() else pd.DataFrame()\n",
    "if 'stock_df' not in globals():\n",
    "    p = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(p) if p.exists() else pd.DataFrame()\n",
    "\n",
    "# Normalize schemas\n",
    "clients_df = clients_df.copy()\n",
    "clients_df.columns = (\n",
    "    clients_df.columns\n",
    "    .str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\".\", \"\", regex=False)\n",
    ")\n",
    "if 'customer_no' in clients_df.columns:\n",
    "    clients_df['customer_no'] = clients_df['customer_no'].astype(str).str.strip()\n",
    "if 'item_no' in clients_df.columns:\n",
    "    clients_df['item_no'] = clients_df['item_no'].astype(str).str.strip()\n",
    "\n",
    "stock_df = stock_df.copy()\n",
    "if 'id' in stock_df.columns:\n",
    "    stock_df['id'] = stock_df['id'].astype(str).str.strip()\n",
    "\n",
    "# critic score column consistency\n",
    "if 'avg_score' not in stock_df.columns and 'AVG' in stock_df.columns:\n",
    "    stock_df.rename(columns={'AVG': 'avg_score'}, inplace=True)\n",
    "\n",
    "# ---------- Loyalty derivation (counts → tier) ----------\n",
    "if {'customer_no','item_no'}.issubset(clients_df.columns):\n",
    "    purchase_counts = clients_df.groupby('customer_no')['item_no'].count().reset_index(name='purchase_count')\n",
    "    purchase_counts['loyalty_level'] = purchase_counts['purchase_count'].apply(\n",
    "        lambda x: 'vip' if x >= 20 else 'gold' if x >= 10 else 'silver' if x >= 5 else 'bronze'\n",
    "    )\n",
    "    clients_df = clients_df.drop(columns=['loyalty_level'], errors='ignore') \\\n",
    "                           .merge(purchase_counts[['customer_no','loyalty_level']], on='customer_no', how='left')\n",
    "else:\n",
    "    clients_df['loyalty_level'] = 'bronze'\n",
    "clients_df['loyalty_level'] = clients_df['loyalty_level'].fillna('bronze')\n",
    "\n",
    "# ---------- Bottle size ML on clients (fallback only) ----------\n",
    "if 'bottle_size' in clients_df.columns:\n",
    "    def _to_ml(x):\n",
    "        try:\n",
    "            v = float(x)\n",
    "            return round(v*10) if v < 100 else v\n",
    "        except:\n",
    "            return np.nan\n",
    "    clients_df['bottle_size_ml'] = clients_df['bottle_size'].apply(_to_ml)\n",
    "else:\n",
    "    clients_df['bottle_size_ml'] = np.nan\n",
    "\n",
    "# Optional sales date\n",
    "sales_dates = clients_df[['customer_no','sales_date']].drop_duplicates() \\\n",
    "    if 'sales_date' in clients_df.columns else pd.DataFrame(columns=['customer_no','sales_date'])\n",
    "\n",
    "# Vintage from stock by item_no (if available)\n",
    "if 'item_no' in clients_df.columns and 'vintage' in stock_df.columns and 'id' in stock_df.columns:\n",
    "    stock_vintage_map = stock_df.set_index('id')['vintage'].astype(str).to_dict()\n",
    "    clients_df['vintage'] = clients_df['item_no'].map(stock_vintage_map)\n",
    "else:\n",
    "    clients_df['vintage'] = 'Unknown'\n",
    "\n",
    "def classify_vintage_group(vint):\n",
    "    try:\n",
    "        s = str(vint).strip().upper()\n",
    "        if s == \"NV\": return \"Non-Vintage Lover\"\n",
    "        y = int(s); cy = datetime.now().year\n",
    "        if y == cy: return \"Likes Current Vintage\"\n",
    "        if y == cy-1: return \"Likes En Primeur\"\n",
    "        if cy - y <= 3: return \"Likes Young Wines\"\n",
    "        if cy - y <= 8: return \"Likes Mature Wines\"\n",
    "        if cy - y > 15: return \"Likes Old Wines\"\n",
    "        return \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "clients_df['inferred_vintage'] = clients_df['vintage'].apply(classify_vintage_group)\n",
    "\n",
    "def classify_size_from_ml(size_ml):\n",
    "    try:\n",
    "        v = int(float(size_ml))\n",
    "        m = {375:\"Half\",750:\"Standard\",1500:\"Magnum\",3000:\"Jeroboam\",4500:\"Rehoboam\",\n",
    "             6000:\"Methuselah\",9000:\"Salmanazar\",12000:\"Balthazar\",15000:\"Nebuchadnezzar\",\n",
    "             18000:\"Melchior\",27000:\"Primat\"}\n",
    "        return m.get(v, \"Unknown\")\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# ---------- Build purchases_df for preference inference ----------\n",
    "needed = ['id','grape_list','type','region','sweetness','body','price_tier','avg_score','vintage','bottle_size_ml']\n",
    "stock_merge_df = stock_df.copy()\n",
    "for c in needed:\n",
    "    if c not in stock_merge_df.columns:\n",
    "        stock_merge_df[c] = np.nan\n",
    "\n",
    "if 'item_no' not in clients_df.columns:\n",
    "    purchases_df = clients_df.copy()\n",
    "    for c in ['grape_list','type','region','sweetness','body','price_tier','avg_score','vintage','bottle_size_ml']:\n",
    "        purchases_df[c] = np.nan\n",
    "else:\n",
    "    purchases_df = clients_df.merge(\n",
    "        stock_merge_df[needed], left_on='item_no', right_on='id', how='left'\n",
    "    )\n",
    "\n",
    "purchases_df['vintage'] = purchases_df.get('vintage', pd.Series([\"Unknown\"]*len(purchases_df))).fillna(\"Unknown\")\n",
    "if 'bottle_size_ml' not in purchases_df.columns:\n",
    "    purchases_df['bottle_size_ml'] = 750\n",
    "purchases_df['size_group'] = purchases_df['bottle_size_ml'].apply(classify_size_from_ml)\n",
    "\n",
    "# Occasion inference (coarse)\n",
    "def infer_occasion(row):\n",
    "    try:\n",
    "        price = str(row.get('price_tier','')).lower()\n",
    "        wine_type = str(row.get('type','')).lower()\n",
    "        size = float(row.get('bottle_size_ml',750))\n",
    "        score = float(row.get('avg_score',0))\n",
    "        sweetness = float(row.get('sweetness',0))\n",
    "        if size >= 3000: return 'Big Event'\n",
    "        if size >= 1500: return 'Celebration'\n",
    "        if 'sparkling' in wine_type or sweetness >= 4: return 'Celebration'\n",
    "        if score >= 92 or (price in ['luxury','ultra luxury','premium'] and size == 750): return 'Gift'\n",
    "        if price in ['budget','entry'] and size <= 750 and sweetness <= 2: return 'Everyday'\n",
    "        if price in ['mid-range','premium'] and 'still' in wine_type: return 'Dinner'\n",
    "        if score >= 90 and price in ['mid-range','premium']: return 'Dinner'\n",
    "        if sweetness <= 2 and score >= 88 and size <= 750: return 'Everyday'\n",
    "        if price in ['mid-range','premium','luxury'] and 'still' in wine_type: return 'Personal Consumption'\n",
    "        return 'Unknown'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "purchases_df['occasion'] = purchases_df.apply(infer_occasion, axis=1)\n",
    "\n",
    "# ---------- Preference aggregation ----------\n",
    "def split_and_flatten(series, delimiter='/'):\n",
    "    return [x.strip() for s in series.dropna() for x in str(s).split(delimiter) if x.strip()]\n",
    "\n",
    "inferred_pref = purchases_df.groupby('customer_no').agg({\n",
    "    'grape_list':  lambda x: ','.join(sorted(set(split_and_flatten(x)))),\n",
    "    'type':        lambda x: ','.join(sorted(set(x.dropna().astype(str).str.lower()))),\n",
    "    'region':      lambda x: ','.join(sorted(set(x.dropna().astype(str).str.lower()))),\n",
    "    'sweetness':   lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'body':        lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'price_tier':  lambda x: x.dropna().mode().iloc[0] if not x.dropna().mode().empty else '',\n",
    "    'avg_score':   lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'vintage':     lambda x: ','.join(sorted(set(x.dropna().astype(str)))),\n",
    "    'size_group':  lambda x: ','.join(sorted(set(x.dropna().astype(str)))),\n",
    "    'occasion':    lambda x: ','.join(sorted(set(x.dropna().astype(str))))\n",
    "}).reset_index().rename(columns={\n",
    "    'grape_list':'inferred_grape_preferences',\n",
    "    'type':'inferred_type',\n",
    "    'region':'inferred_region',\n",
    "    'sweetness':'inferred_sweetness',\n",
    "    'body':'inferred_body',\n",
    "    'price_tier':'inferred_budget',\n",
    "    'avg_score':'avg_critic_score',\n",
    "    'vintage':'inferred_vintage',\n",
    "    'size_group':'inferred_size',\n",
    "    'occasion':'inferred_occasion'\n",
    "})\n",
    "\n",
    "# Fallback vintage/size preferences when inference is missing\n",
    "clients_df['fallback_size_label'] = clients_df['bottle_size_ml'].apply(classify_size_from_ml)\n",
    "fallback_vintage = (\n",
    "    clients_df.groupby('customer_no')['vintage']\n",
    "    .agg(lambda x: x.dropna().mode().iloc[0] if not x.dropna().mode().empty else 'Unknown')\n",
    "    .reset_index(name='fallback_vintage')\n",
    ")\n",
    "fallback_vintage['fallback_vintage_label'] = fallback_vintage['fallback_vintage'].apply(classify_vintage_group)\n",
    "\n",
    "inferred_pref = inferred_pref.merge(\n",
    "    fallback_vintage[['customer_no','fallback_vintage_label']], on='customer_no', how='left'\n",
    ")\n",
    "inferred_pref['inferred_vintage'] = inferred_pref['fallback_vintage_label'].fillna(inferred_pref['inferred_vintage'])\n",
    "inferred_pref.drop(columns=['fallback_vintage_label'], inplace=True)\n",
    "\n",
    "inferred_pref = inferred_pref.merge(\n",
    "    clients_df[['customer_no','fallback_size_label']].drop_duplicates(),\n",
    "    on='customer_no', how='left'\n",
    ")\n",
    "inferred_pref['inferred_size'] = inferred_pref['fallback_size_label'].fillna(inferred_pref['inferred_size'])\n",
    "inferred_pref.drop(columns=['fallback_size_label'], inplace=True)\n",
    "\n",
    "# Merge traits + loyalty + sales date\n",
    "client_pref_df = (inferred_pref\n",
    "    .merge(clients_df[['customer_no']].drop_duplicates(), on='customer_no', how='right')\n",
    "    .merge(clients_df[['customer_no','loyalty_level']].drop_duplicates(), on='customer_no', how='left')\n",
    "    .merge(sales_dates, on='customer_no', how='left')\n",
    ")\n",
    "client_pref_df['loyalty_level'] = client_pref_df['loyalty_level'].fillna('bronze')\n",
    "\n",
    "# Flags\n",
    "client_pref_df['prefers_high_scores'] = (coerce_numeric(client_pref_df['avg_critic_score']).fillna(0) >= 95)\n",
    "stock_df['avg_score'] = coerce_numeric(stock_df.get('avg_score', np.nan))\n",
    "stock_df['high_score'] = stock_df['avg_score'].ge(95)\n",
    "\n",
    "# ---------- UI Filters (from Cell 1 or env) ----------\n",
    "if 'filters' not in globals():\n",
    "    try:\n",
    "        filters = json.loads(os.getenv(\"FILTER_INPUTS\", \"{}\"))\n",
    "    except Exception:\n",
    "        filters = {}\n",
    "\n",
    "# Loyalty filter (client subset)\n",
    "loy = str(filters.get('loyalty','all') or 'all').lower()\n",
    "if loy != 'all':\n",
    "    before = len(client_pref_df)\n",
    "    client_pref_df = client_pref_df.loc[client_pref_df['loyalty_level'].str.lower() == loy].copy()\n",
    "    after = len(client_pref_df)\n",
    "    print(f\"🎯 Loyalty filter [{loy}] → clients: {before} → {after}\")\n",
    "\n",
    "# Wine type filter (stock subset)\n",
    "wt = (filters.get('wine_type') or '').strip().lower()\n",
    "if wt:\n",
    "    if wt in {'red','white','rose','rosé','sparkling','dessert'}:\n",
    "        if wt in {'rose','rosé'}:\n",
    "            mask = stock_df.get('color','').astype(str).str.contains('ros', case=False, na=False)\n",
    "        elif wt == 'sparkling':\n",
    "            mask = stock_df.get('type','').astype(str).str.contains('sparkling', case=False, na=False) \\\n",
    "                   | stock_df.get('color','').astype(str).str.contains('spark', case=False, na=False)\n",
    "        elif wt == 'dessert':\n",
    "            mask = stock_df.get('type','').astype(str).str.contains('dessert', case=False, na=False)\n",
    "        else:\n",
    "            mask = stock_df.get('color','').astype(str).str.contains(wt, case=False, na=False)\n",
    "        before = len(stock_df)\n",
    "        stock_df = stock_df.loc[mask].copy()\n",
    "        after = len(stock_df)\n",
    "        print(f\"🍷 Wine type [{wt}] → wines: {before} → {after}\")\n",
    "\n",
    "# Bottle size filter\n",
    "bs = filters.get('bottle_size', None)\n",
    "try:\n",
    "    bs_ml = int(float(bs)) if bs is not None else None\n",
    "except Exception:\n",
    "    bs_ml = None\n",
    "if bs_ml:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[(coerce_numeric(stock_df.get('bottle_size_ml')).round(0) == bs_ml)].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"🍾 Bottle size [{bs_ml}ml] → wines: {before} → {after}\")\n",
    "\n",
    "# Price tier bucket filter\n",
    "pt = (filters.get('price_tier_bucket') or '').strip()\n",
    "if pt:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[stock_df.get('price_tier','').astype(str).str.lower() == pt.lower()].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"💰 Price tier [{pt}] → wines: {before} → {after}\")\n",
    "\n",
    "# Low stock filter\n",
    "if filters.get('last_stock', False):\n",
    "    th = int(filters.get('last_stock_threshold', 10) or 10)\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[coerce_numeric(stock_df.get('stock')).fillna(0) < th].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"⚠️ Last stock < {th} → wines: {before} → {after}\")\n",
    "\n",
    "# Seasonality filter (only if column exists)\n",
    "if filters.get(\"seasonality_boost\", False) and \"seasonality_boost\" in stock_df.columns:\n",
    "    stock_df[\"seasonal_match\"] = stock_df[\"seasonality_boost\"].apply(\n",
    "        lambda x: selected_season in x if isinstance(x, list) else False\n",
    "    )\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[stock_df[\"seasonal_match\"] == True].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"🌿 Seasonality [{selected_season}] → wines: {before} → {after}\")\n",
    "elif filters.get(\"seasonality_boost\", False):\n",
    "    print(\"⚠️ 'seasonality_boost' column not found — skipping seasonality filter\")\n",
    "\n",
    "# Recency filter (avoid using items that were already used last year in same week, or this week if already emitted)\n",
    "def _load_calendar_ids(p: Path) -> set:\n",
    "    try:\n",
    "        if p.suffix.lower() == \".pkl\":\n",
    "            obj = pd.read_pickle(p)\n",
    "            cal = obj.get(\"weekly_calendar\", obj) if isinstance(obj, dict) else obj\n",
    "        else:\n",
    "            cal = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            cal = cal.get(\"weekly_calendar\", cal)\n",
    "        return extract_ids_from_weekly_calendar(cal)\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "candidates = [\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year-1}_week_{week_number}.pkl\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year-1}_week_{week_number}.json\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year}_week_{week_number}.pkl\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year}_week_{week_number}.json\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_week_{week_number}.pkl\",   # legacy\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_week_{week_number}.json\",   # legacy\n",
    "]\n",
    "past_ids = set()\n",
    "used_path = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        ids = _load_calendar_ids(p)\n",
    "        if ids:\n",
    "            past_ids = ids\n",
    "            used_path = p\n",
    "            break\n",
    "\n",
    "if past_ids:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[~stock_df['id'].astype(str).isin(past_ids)].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"⏮️ Recency filter → removed {before - after} (source: {used_path.name})\")\n",
    "else:\n",
    "    print(f\"📁 No past schedule for Week {week_number} → skipping recency filter\")\n",
    "\n",
    "# Refresh flags after filters\n",
    "stock_df['avg_score'] = coerce_numeric(stock_df.get('avg_score'))\n",
    "stock_df['high_score'] = stock_df['avg_score'].ge(95)\n",
    "\n",
    "# ---------- CPI compute ----------\n",
    "display_col = 'wine' if 'wine' in stock_df.columns else 'id'\n",
    "\n",
    "def compute_cpi_matrix(client_df, stock_df, style=\"default\", display_col='wine'):\n",
    "    stock_df = stock_df.copy()\n",
    "    stock_df['id'] = stock_df['id'].astype(str)\n",
    "\n",
    "    # Baseline weights\n",
    "    weights = {\n",
    "        'grape': 1.0, 'type': 1.0, 'region': 1.0,\n",
    "        'sweetness': 0.5, 'body': 0.5,\n",
    "        'budget': 0.75, 'prefers_high_scores': 0.75,\n",
    "        'avg_score': 0.5\n",
    "    }\n",
    "    loyalty_bonus = {'bronze': 0.0, 'silver': 0.25, 'gold': 0.5, 'vip': 0.75}\n",
    "\n",
    "    # Persona tweaks\n",
    "    st = (style or 'default').lower()\n",
    "    if st == \"cat\":\n",
    "        weights['type'] = 1.2\n",
    "        weights['region'] = 1.2\n",
    "        weights['avg_score'] = 0.4\n",
    "    elif st == \"nigo\":\n",
    "        weights['budget'] = 1.0\n",
    "        weights['avg_score'] = 0.7\n",
    "\n",
    "    total_possible = sum(weights.values()) + max(loyalty_bonus.values())\n",
    "\n",
    "    # Ensure numeric for tolerance checks\n",
    "    sd_sweet = coerce_numeric(stock_df.get('sweetness')).fillna(-999)\n",
    "    sd_body  = coerce_numeric(stock_df.get('body')).fillna(-999)\n",
    "\n",
    "    result = []\n",
    "    it = client_df.iterrows() if len(client_df) else iter([(-1, {'customer_no':'GLOBAL','inferred_grape_preferences':'','inferred_type':'','inferred_region':'','inferred_sweetness':np.nan,'inferred_body':np.nan,'inferred_budget':'','avg_critic_score':np.nan,'loyalty_level':'bronze','prefers_high_scores':False})])\n",
    "\n",
    "    for _, client in tqdm(it, total=max(len(client_df),1), desc=\"🔄 Generating CPI vectors\"):\n",
    "        score = pd.Series(0.0, index=stock_df.index)\n",
    "\n",
    "        # grapes\n",
    "        grape_prefs = set(str(client.get('inferred_grape_preferences','')).lower().split(','))\n",
    "        wine_grapes = stock_df.get('grape_list','').fillna('').astype(str).str.lower().str.split('/')\n",
    "        score += wine_grapes.apply(lambda gs: any(g.strip() in grape_prefs for g in gs if g)).astype(float) * weights['grape']\n",
    "\n",
    "        # type/region\n",
    "        score += (stock_df.get('type','').fillna('').astype(str).str.lower()\n",
    "                  .str.contains(str(client.get('inferred_type','')).lower(), na=False)) * weights['type']\n",
    "        score += (stock_df.get('region','').fillna('').astype(str).str.lower()\n",
    "                  .str.contains(str(client.get('inferred_region','')).lower(), na=False)) * weights['region']\n",
    "\n",
    "        # sweetness/body (tolerance)\n",
    "        cs = client.get('inferred_sweetness', np.nan)\n",
    "        cb = client.get('inferred_body', np.nan)\n",
    "        if pd.notna(cs):\n",
    "            score += (np.isclose(sd_sweet, float(cs), atol=0.5)).astype(float) * weights['sweetness']\n",
    "        if pd.notna(cb):\n",
    "            score += (np.isclose(sd_body,  float(cb), atol=0.5)).astype(float) * weights['body']\n",
    "\n",
    "        # budget match\n",
    "        score += (stock_df.get('price_tier','').fillna('').astype(str).str.lower()\n",
    "                  == str(client.get('inferred_budget','')).lower()) * weights['budget']\n",
    "\n",
    "        # high score preference\n",
    "        pref_hi = bool(client.get('prefers_high_scores', False))\n",
    "        score += (stock_df['high_score'] & pref_hi).astype(float) * weights['prefers_high_scores']\n",
    "\n",
    "        # general avg score quality\n",
    "        score += (coerce_numeric(stock_df['avg_score']).fillna(0) >= 90).astype(float) * weights['avg_score']\n",
    "\n",
    "        # loyalty\n",
    "        score += loyalty_bonus.get(str(client.get('loyalty_level','bronze')).lower(), 0)\n",
    "\n",
    "        score /= total_possible\n",
    "        result.append(score.round(4).rename(f\"pref_cpi_for_{client.get('customer_no','GLOBAL')}\"))\n",
    "\n",
    "    matrix = pd.concat([stock_df[['id', display_col]].reset_index(drop=True)] + result, axis=1)\n",
    "    return matrix\n",
    "\n",
    "style = (filters.get('style') or 'default').lower()\n",
    "t0 = perf_counter()\n",
    "cpi_matrix = compute_cpi_matrix(client_pref_df, stock_df, style=style, display_col=display_col)\n",
    "print(\"⏱️ CPI computation completed in\", round(perf_counter() - t0, 2), \"seconds.\")\n",
    "\n",
    "# ---------- Attach average CPI per wine BEFORE saving UI snapshot ----------\n",
    "cpi_cols = [c for c in cpi_matrix.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "if cpi_cols:\n",
    "    cpi_avg_df = pd.DataFrame({\n",
    "        \"id\": cpi_matrix[\"id\"].astype(str),\n",
    "        \"avg_cpi_score\": cpi_matrix[cpi_cols].mean(axis=1).round(4)\n",
    "    })\n",
    "    stock_df = stock_df.merge(cpi_avg_df, on=\"id\", how=\"left\")\n",
    "else:\n",
    "    stock_df[\"avg_cpi_score\"] = np.nan\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "client_pref_df.to_pickle(OUTPUT_PATH / \"client_pref_df_latest.pkl\")\n",
    "cpi_matrix.to_pickle(OUTPUT_PATH / \"cpi_matrix_latest.pkl\")\n",
    "\n",
    "# a compact stock file the webapp can use to render cards\n",
    "ui_cols = [\n",
    "    'id','wine','producer','vintage','price_tier','stock','full_type','type','color',\n",
    "    'region','region_group','bottle_size_ml','avg_score','high_score','avg_cpi_score'\n",
    "]\n",
    "present = [c for c in ui_cols if c in stock_df.columns]\n",
    "stock_df[present].to_pickle(OUTPUT_PATH / \"stock_for_ui_latest.pkl\")\n",
    "\n",
    "print(\"✅ Preferences, CPI matrix, and UI stock snapshot saved (.pkl).\")\n",
    "print(\"🧪 CPI Matrix shape:\", cpi_matrix.shape, \"| UI stock cols:\", present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e403bf5-384f-4b9d-a4ad-6def25b55281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:27.657483Z",
     "iopub.status.busy": "2025-09-05T06:55:27.657483Z",
     "iopub.status.idle": "2025-09-05T06:55:32.588712Z",
     "shell.execute_reply": "2025-09-05T06:55:32.583709Z"
    },
    "papermill": {
     "duration": 4.935236,
     "end_time": "2025-09-05T06:55:32.589713",
     "exception": false,
     "start_time": "2025-09-05T06:55:27.654477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Fallback pool after filters: 731 rows\n",
      "💾 Saved fallback_pool → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\fallback_pool.pkl (top 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wine</th>\n",
       "      <th>vintage</th>\n",
       "      <th>stock</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>price_tier</th>\n",
       "      <th>full_type</th>\n",
       "      <th>region_group</th>\n",
       "      <th>bottle_size_ml</th>\n",
       "      <th>avg_cpi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62139</td>\n",
       "      <td>Cabernet Sauvignon Pritchard Hill</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>750</td>\n",
       "      <td>0.535446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57376</td>\n",
       "      <td>Cabernet Sauvignon Helena Montana</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>Premium</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>750</td>\n",
       "      <td>0.535446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60057</td>\n",
       "      <td>Melbury</td>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>Ultra Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>750</td>\n",
       "      <td>0.535446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60582</td>\n",
       "      <td>La Gaffelière</td>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>94.846154</td>\n",
       "      <td>Mid-range</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>St. Emilion</td>\n",
       "      <td>750</td>\n",
       "      <td>0.535446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64332</td>\n",
       "      <td>Le Pin</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>94.607143</td>\n",
       "      <td>Ultra Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Pomerol</td>\n",
       "      <td>750</td>\n",
       "      <td>0.535446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                               wine vintage  stock  avg_score  \\\n",
       "0  62139  Cabernet Sauvignon Pritchard Hill    2021      3  97.000000   \n",
       "1  57376  Cabernet Sauvignon Helena Montana    2018      3  95.000000   \n",
       "2  60057                            Melbury    2020      3  95.000000   \n",
       "3  60582                      La Gaffelière    2023      6  94.846154   \n",
       "4  64332                             Le Pin    2024      9  94.607143   \n",
       "\n",
       "     price_tier  full_type region_group  bottle_size_ml   avg_cpi  \n",
       "0        Luxury  Still Red  Napa Valley             750  0.535446  \n",
       "1       Premium  Still Red  Napa Valley             750  0.535446  \n",
       "2  Ultra Luxury  Still Red  Napa Valley             750  0.535446  \n",
       "3     Mid-range  Still Red  St. Emilion             750  0.535446  \n",
       "4  Ultra Luxury  Still Red      Pomerol             750  0.535446  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 7: Build/refresh fallback_pool safely ---\n",
    "# Goal: give the scheduler a good \"pool\" to draw from when client prefs can't fill the week.\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Fallbacks if prior cells didn't run ----------\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "except NameError:\n",
    "    _default_output = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "    OUTPUT_PATH = Path(os.getenv(\"OUTPUT_PATH\", _default_output))\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'stock_df' not in globals():\n",
    "    _stock_pkl = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(_stock_pkl) if _stock_pkl.exists() else pd.DataFrame()\n",
    "\n",
    "if 'id' not in stock_df.columns:\n",
    "    stock_df['id'] = \"\"  # ensure merge key exists\n",
    "\n",
    "# ---------- Guards: ensure filters + UI vars exist even if Cell 5/6 didn't run ----------\n",
    "# 1) Ensure `filters` is a dict (load from file/env if missing)\n",
    "if 'filters' not in globals() or not isinstance(filters, dict):\n",
    "    filters = {}\n",
    "    _filters_path = os.path.join(\"notebooks\", \"filters.json\")\n",
    "    if os.path.exists(_filters_path):\n",
    "        try:\n",
    "            with open(_filters_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                filters = json.load(f) or {}\n",
    "        except Exception as _e:\n",
    "            print(f\"⚠️ Could not read filters.json: {_e}\")\n",
    "    else:\n",
    "        try:\n",
    "            env_f = os.getenv(\"FILTER_INPUTS\", \"\")\n",
    "            if env_f:\n",
    "                filters = json.loads(env_f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 2) Define UI variables with robust defaults\n",
    "if 'selected_type' not in globals():\n",
    "    selected_type = filters.get(\"wine_type\", None)\n",
    "\n",
    "if 'selected_size' not in globals():\n",
    "    _sz = filters.get(\"bottle_size\", None)\n",
    "    try:\n",
    "        selected_size = int(_sz) if _sz is not None and str(_sz).strip().lower() != \"bigger\" else None\n",
    "    except Exception:\n",
    "        selected_size = None\n",
    "\n",
    "if 'last_stock' not in globals():\n",
    "    _ls = filters.get(\"last_stock\", False)\n",
    "    last_stock = (str(_ls).strip().lower() in (\"1\", \"true\", \"yes\", \"y\"))\n",
    "\n",
    "if 'seasonality_boost' not in globals():\n",
    "    _sb = filters.get(\"seasonality_boost\", False)\n",
    "    seasonality_boost = (str(_sb).strip().lower() in (\"1\", \"true\", \"yes\", \"y\"))\n",
    "\n",
    "def _parse_size_to_ml(val):\n",
    "    try:\n",
    "        s = str(val).strip().lower().replace(\"ml\",\"\").replace(\"cl\",\"\").replace(\"l\",\"\")\n",
    "        if not s: return np.nan\n",
    "        v = float(s)\n",
    "        if v <= 100:  # cl\n",
    "            return int(round(v * 10))\n",
    "        if v < 20:    # liters\n",
    "            return int(round(v * 1000))\n",
    "        return int(round(v))  # already ml\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ---------- Try CPI-augmented pool; else fall back to stock only ----------\n",
    "try:\n",
    "    if \"merged_cpi_df\" in globals():\n",
    "        _cpi_src = merged_cpi_df.copy()\n",
    "        cpi_cols = [c for c in _cpi_src.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "        _avg = _cpi_src[cpi_cols].mean(axis=1) if cpi_cols else pd.Series(np.nan, index=_cpi_src.index)\n",
    "        cpi_avg_df = _cpi_src[['id']].copy()\n",
    "        cpi_avg_df['avg_cpi'] = _avg\n",
    "    else:\n",
    "        _cpi_path = OUTPUT_PATH / \"cpi_matrix_latest.pkl\"\n",
    "        _cpi = pd.read_pickle(_cpi_path) if _cpi_path.exists() else pd.DataFrame()\n",
    "        if _cpi.empty or 'id' not in _cpi.columns:\n",
    "            raise FileNotFoundError(\"cpi_matrix_latest.pkl missing or malformed\")\n",
    "        cpi_cols = [c for c in _cpi.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "        _avg = _cpi[cpi_cols].mean(axis=1) if cpi_cols else pd.Series(np.nan, index=_cpi.index)\n",
    "        cpi_avg_df = pd.DataFrame({\"id\": _cpi['id'].astype(str), \"avg_cpi\": _avg.values})\n",
    "\n",
    "    # Assemble base pool from stock with useful columns\n",
    "    _stock_cols = ['id','wine','vintage','stock','avg_score','price_tier','full_type','region_group']\n",
    "    have_cols = [c for c in _stock_cols if c in stock_df.columns]\n",
    "    base = stock_df[have_cols].copy()\n",
    "    base['id'] = base['id'].astype(str)\n",
    "\n",
    "    # Bottle size to ML (prefer existing column)\n",
    "    if 'bottle_size_ml' in stock_df.columns:\n",
    "        base = base.merge(stock_df[['id','bottle_size_ml']], on='id', how='left')\n",
    "    else:\n",
    "        size_sources = [c for c in stock_df.columns if c.lower() in ('size','size_cl','bottle_size','bottle size')]\n",
    "        if size_sources:\n",
    "            _sz = stock_df[['id', size_sources[0]]].rename(columns={size_sources[0]: 'size_raw'})\n",
    "            _sz['bottle_size_ml'] = _sz['size_raw'].apply(_parse_size_to_ml)\n",
    "            base = base.merge(_sz[['id','bottle_size_ml']], on='id', how='left')\n",
    "        else:\n",
    "            base['bottle_size_ml'] = np.nan\n",
    "\n",
    "    # Attach CPI average\n",
    "    fallback_pool = base.merge(cpi_avg_df, on='id', how='left')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not build CPI-based fallback pool: {e}\")\n",
    "    base_cols = ['id','wine','vintage','stock','avg_score','price_tier','full_type','region_group']\n",
    "    have_cols = [c for c in base_cols if c in stock_df.columns]\n",
    "    if 'id' not in have_cols:\n",
    "        have_cols = ['id'] + have_cols\n",
    "    fallback_pool = stock_df[have_cols].copy()\n",
    "    if 'bottle_size_ml' in stock_df.columns and 'bottle_size_ml' not in fallback_pool.columns:\n",
    "        fallback_pool = fallback_pool.merge(stock_df[['id','bottle_size_ml']], on='id', how='left')\n",
    "    fallback_pool['avg_cpi'] = np.nan  # ensure column exists\n",
    "\n",
    "# ---------- Ensure dtypes ----------\n",
    "fallback_pool['stock']     = pd.to_numeric(fallback_pool.get('stock', 0), errors='coerce').fillna(0).astype(int)\n",
    "fallback_pool['avg_cpi']   = pd.to_numeric(fallback_pool.get('avg_cpi', np.nan), errors='coerce')\n",
    "fallback_pool['avg_score'] = pd.to_numeric(fallback_pool.get('avg_score', np.nan), errors='coerce')\n",
    "\n",
    "# If CPI avg came from Cell 6 as avg_cpi_score, use it\n",
    "if ('avg_cpi' not in fallback_pool.columns) or fallback_pool['avg_cpi'].isna().all():\n",
    "    if 'avg_cpi_score' in stock_df.columns:\n",
    "        try:\n",
    "            fallback_pool = fallback_pool.merge(\n",
    "                stock_df[['id','avg_cpi_score']].rename(columns={'avg_cpi_score':'avg_cpi'}),\n",
    "                on='id', how='left'\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- Apply UI filters ----------\n",
    "_selected_type = (selected_type or \"\").strip()\n",
    "_selected_size = selected_size\n",
    "_last_stock    = bool(last_stock)\n",
    "_season_boost  = bool(seasonality_boost)\n",
    "\n",
    "# wine type\n",
    "if _selected_type and _selected_type.lower() != \"all\":\n",
    "    if 'full_type' in fallback_pool.columns:\n",
    "        mask = fallback_pool['full_type'].astype(str).str.contains(_selected_type, case=False, na=False)\n",
    "        fallback_pool = fallback_pool[mask]\n",
    "    else:\n",
    "        mask = False\n",
    "        for c in [c for c in fallback_pool.columns if c.lower() in ('type','color','full_type')]:\n",
    "            mask = mask | fallback_pool[c].astype(str).str.contains(_selected_type, case=False, na=False)\n",
    "        fallback_pool = fallback_pool[mask]\n",
    "\n",
    "# bottle size (ml)\n",
    "if _selected_size:\n",
    "    try:\n",
    "        sel_ml = int(_selected_size)\n",
    "        fallback_pool = fallback_pool[ fallback_pool['bottle_size_ml'].fillna(-1).astype(int) == sel_ml ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# last stock (<10)\n",
    "if _last_stock:\n",
    "    fallback_pool = fallback_pool[fallback_pool['stock'] < 10]\n",
    "\n",
    "# seasonality boost (only if column exists and is truthy)\n",
    "if _season_boost and 'seasonality_boost' in stock_df.columns:\n",
    "    season_ids = set(\n",
    "        stock_df.loc[\n",
    "            stock_df['seasonality_boost'].apply(lambda x: bool(x) and str(x).strip() not in ('[]','False','false','0')),\n",
    "            'id'\n",
    "        ].astype(str)\n",
    "    )\n",
    "    fallback_pool = fallback_pool[fallback_pool['id'].astype(str).isin(season_ids)]\n",
    "\n",
    "print(f\"🧮 Fallback pool after filters: {len(fallback_pool)} rows\")\n",
    "\n",
    "# ---------- Rank (prefer higher CPI, then score, then stock) ----------\n",
    "if 'avg_cpi' not in fallback_pool.columns:\n",
    "    fallback_pool['avg_cpi'] = np.nan\n",
    "fallback_pool['avg_cpi']   = pd.to_numeric(fallback_pool['avg_cpi'], errors='coerce')\n",
    "fallback_pool['avg_score'] = pd.to_numeric(fallback_pool['avg_score'], errors='coerce')\n",
    "\n",
    "fallback_pool = fallback_pool.sort_values(\n",
    "    ['avg_cpi','avg_score','stock'], ascending=[False, False, False], na_position='last'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Persist for the webapp/scheduler\n",
    "fallback_path = OUTPUT_PATH / \"fallback_pool.pkl\"\n",
    "fallback_pool.to_pickle(fallback_path)\n",
    "print(f\"💾 Saved fallback_pool → {fallback_path} (top {min(5, len(fallback_pool))} rows):\")\n",
    "display(fallback_pool.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839af05a-c816-4f39-8c01-489ed6c11c1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:32.590713Z",
     "iopub.status.busy": "2025-09-05T06:55:32.590713Z",
     "iopub.status.idle": "2025-09-05T06:55:33.123778Z",
     "shell.execute_reply": "2025-09-05T06:55:33.123778Z"
    },
    "papermill": {
     "duration": 0.534065,
     "end_time": "2025-09-05T06:55:33.123778",
     "exception": false,
     "start_time": "2025-09-05T06:55:32.589713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ top3_by_type is empty — using 'Unknown' defaults for region/type.\n",
      "🔍 Effective UI filters:\n",
      " {\n",
      "  \"loyalty\": \"all\",\n",
      "  \"wine_type\": null,\n",
      "  \"bottle_size\": 750,\n",
      "  \"price_tier_bucket\": \"\",\n",
      "  \"last_stock\": true,\n",
      "  \"last_stock_threshold\": 10,\n",
      "  \"seasonality_boost\": false,\n",
      "  \"style\": \"default\",\n",
      "  \"calendar_day\": null\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved year+week UI files for 2025-W36 in C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\n"
     ]
    }
   ],
   "source": [
    "# ---  CELL 8: Save UI-Ready JSON & PKL for Flask ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "# ---------- Fallbacks if earlier cells didn't run ----------\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "except NameError:\n",
    "    _default_output = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "    OUTPUT_PATH = Path(os.getenv(\"OUTPUT_PATH\", _default_output))\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    NUM_SLOTS\n",
    "except NameError:\n",
    "    NUM_SLOTS = 5\n",
    "\n",
    "# Load stock if not in memory\n",
    "if 'stock_df' not in globals():\n",
    "    _stock_pkl = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(_stock_pkl) if _stock_pkl.exists() else pd.DataFrame()\n",
    "\n",
    "# Load client pref if not in memory\n",
    "if 'client_pref_df' not in globals():\n",
    "    _pref_pkl = OUTPUT_PATH / \"client_pref_df_latest.pkl\"\n",
    "    client_pref_df = pd.read_pickle(_pref_pkl) if _pref_pkl.exists() else pd.DataFrame(columns=['customer_no'])\n",
    "\n",
    "# ---------- Locked snapshot helpers ----------\n",
    "DAYS = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _locked_to_df(locked_json: dict, stock_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for day in DAYS:\n",
    "        for it in (locked_json.get(day) or []):\n",
    "            if not it: \n",
    "                continue\n",
    "            rows.append({\"day\": day, **it})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"id\"] = df.get(\"id\",\"\").astype(str)\n",
    "    enrich_cols = [\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"avg_cpi_score\"]\n",
    "    have = [c for c in enrich_cols if c in stock_df.columns]\n",
    "    if have:\n",
    "        df = df.merge(stock_df[have].drop_duplicates(\"id\"), on=\"id\", how=\"left\")\n",
    "    df[\"locked\"] = True\n",
    "    # Title-case price_tier if present; otherwise create an empty column\n",
    "    if \"price_tier\" in df.columns:\n",
    "        df[\"price_tier\"] = df[\"price_tier\"].apply(\n",
    "            lambda x: x.title().strip() if isinstance(x, str) and x else \"\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"price_tier\"] = \"\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Determine selected year/week and load locked snapshot ----------\n",
    "try:\n",
    "    sel_year = int(calendar_year)\n",
    "except Exception:\n",
    "    sel_year = datetime.now().year\n",
    "try:\n",
    "    sel_week = int(week_number)\n",
    "except Exception:\n",
    "    sel_week = datetime.now().isocalendar().week\n",
    "\n",
    "LOCKED_PATH = OUTPUT_PATH / \"locked_weeks\"\n",
    "LOCKED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "persisted = _load_json(LOCKED_PATH / f\"locked_calendar_{sel_year}_week_{sel_week}.json\")\n",
    "\n",
    "# Prefer in-memory structured snapshot from Cell 1; else persisted; else empty\n",
    "if \"locked_calendar_snapshot\" in globals() and locked_calendar_snapshot:\n",
    "    # Convert structured snapshot {day:{main,overflow}} → flat lists per day\n",
    "    locked_json = {d: (locked_calendar_snapshot.get(d, {}).get(\"main\") or []) for d in DAYS}\n",
    "elif persisted:\n",
    "    locked_json = persisted\n",
    "else:\n",
    "    locked_json = {d: [] for d in DAYS}\n",
    "\n",
    "# Now that we have locked_json and stock_df, build locked_df\n",
    "locked_df = _locked_to_df(locked_json, stock_df)\n",
    "\n",
    "# ---------- Tier helpers ----------\n",
    "TIER_MAP_NAME = {\n",
    "    \"budget\": \"Budget\",\n",
    "    \"entry\": \"Budget\",\n",
    "    \"mid\": \"Mid-range\",\n",
    "    \"mid_range\": \"Mid-range\",\n",
    "    \"mid-range\": \"Mid-range\",\n",
    "    \"premium\": \"Premium\",\n",
    "    \"luxury\": \"Luxury\",\n",
    "    \"ultra\": \"Ultra Luxury\",\n",
    "    \"ultra-luxury\": \"Ultra Luxury\",\n",
    "    \"ultra_luxury\": \"Ultra Luxury\",\n",
    "    \"ultra luxury\": \"Ultra Luxury\",\n",
    "}\n",
    "TIER_MAP_ID = {\n",
    "    \"Budget\": \"budget\",\n",
    "    \"Mid-range\": \"mid\",\n",
    "    \"Premium\": \"premium\",\n",
    "    \"Luxury\": \"luxury\",\n",
    "    \"Ultra Luxury\": \"ultra\",\n",
    "}\n",
    "\n",
    "def canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    key = s.lower().replace(\" \", \"_\").replace(\"__\", \"_\").replace(\"-\", \"_\")\n",
    "    return TIER_MAP_NAME.get(key, s)\n",
    "\n",
    "def tier_id_from_name(name: str) -> str:\n",
    "    return TIER_MAP_ID.get(canon_tier_name(name), \"\")\n",
    "\n",
    "def to_ml(size_val):\n",
    "    \"\"\"Accepts '750', '75cl', '1.5L', 750 etc. -> returns int ml or np.nan\"\"\"\n",
    "    if size_val is None:\n",
    "        return np.nan\n",
    "    s = str(size_val).strip().lower()\n",
    "    try:\n",
    "        if s.endswith(\"ml\"):\n",
    "            return int(float(s[:-2]))\n",
    "        if s.endswith(\"cl\"):\n",
    "            return int(float(s[:-2]) * 10)\n",
    "        if s.endswith(\"l\"):\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        # plain number: if < 100 -> assume cl, else ml\n",
    "        v = float(s)\n",
    "        return int(v * 10) if v < 100 else int(v)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: dict):\n",
    "    \"\"\"Ensure columns exist with default; cols = {name: default}\"\"\"\n",
    "    for c, default in cols.items():\n",
    "        if c not in df.columns:\n",
    "            df[c] = default\n",
    "    return df\n",
    "\n",
    "# ---------- Default region/type from any top picks (optional) ----------\n",
    "default_region = \"Unknown\"\n",
    "default_type   = \"Unknown\"\n",
    "\n",
    "if 'top3_by_type' in globals() and isinstance(top3_by_type, pd.DataFrame) and not top3_by_type.empty:\n",
    "    top_rec = top3_by_type.iloc[0]\n",
    "    wine_id = str(top_rec.get('id', \"\")).strip()\n",
    "    default_type = str(top_rec.get('full_type', 'Unknown')) or 'Unknown'\n",
    "    try:\n",
    "        stock_df_ids = stock_df.copy()\n",
    "        stock_df_ids['id'] = stock_df_ids['id'].astype(str)\n",
    "        match_row = stock_df_ids.loc[stock_df_ids['id'] == wine_id, 'region_group']\n",
    "        if not match_row.empty:\n",
    "            default_region = str(match_row.iloc[0]) or \"Unknown\"\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"⚠️ top3_by_type is empty — using 'Unknown' defaults for region/type.\")\n",
    "\n",
    "# Build seg_df filling missing values with defaults (but not overwriting existing)\n",
    "seg_df = client_pref_df.copy()\n",
    "seg_df['region_group'] = seg_df.get('region_group', pd.Series(index=seg_df.index, dtype='object')).fillna(default_region)\n",
    "seg_df['full_type']    = seg_df.get('full_type',    pd.Series(index=seg_df.index, dtype='object')).fillna(default_type)\n",
    "\n",
    "# ---------- Load UI filters (file -> env fallback -> defaults) ----------\n",
    "filters_path = Path(\"notebooks\") / \"filters.json\"\n",
    "filters = {}\n",
    "try:\n",
    "    if filters_path.exists():\n",
    "        filters = json.loads(filters_path.read_text(encoding=\"utf-8\")) or {}\n",
    "    elif os.getenv(\"FILTER_INPUTS\"):\n",
    "        filters = json.loads(os.getenv(\"FILTER_INPUTS\"))\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Failed to read filters.json/env: {e}\")\n",
    "finally:\n",
    "    if not filters:\n",
    "        filters = {\n",
    "            \"last_stock\": False,\n",
    "            \"seasonality_boost\": False,\n",
    "            \"wine_type\": None,\n",
    "            \"bottle_size\": None,\n",
    "            \"price_tier_bucket\": \"\",\n",
    "            \"price_tiers\": [],\n",
    "            \"loyalty_levels\": []\n",
    "        }\n",
    "\n",
    "print(\"🔍 Effective UI filters:\\n\", json.dumps(filters, indent=2))\n",
    "\n",
    "# ---------- Normalize stock needed columns ----------\n",
    "stock_df = stock_df.copy()\n",
    "stock_df['id']    = stock_df.get('id', \"\").astype(str)\n",
    "stock_df['stock'] = pd.to_numeric(stock_df.get('stock', 0), errors='coerce').fillna(0).astype(int)\n",
    "stock_df['price_tier'] = stock_df.get('price_tier', \"\").map(canon_tier_name)\n",
    "stock_df['price_tier_id'] = stock_df['price_tier'].map(tier_id_from_name)\n",
    "\n",
    "if 'bottle_size_ml' not in stock_df.columns:\n",
    "    if 'size' in stock_df.columns:\n",
    "        stock_df['bottle_size_ml'] = stock_df['size'].apply(to_ml)\n",
    "    elif 'size_cl' in stock_df.columns:\n",
    "        stock_df['bottle_size_ml'] = pd.to_numeric(stock_df['size_cl'], errors='coerce') * 10\n",
    "    else:\n",
    "        stock_df['bottle_size_ml'] = np.nan\n",
    "\n",
    "stock_df = ensure_cols(stock_df, {'occasion': 'Unknown'})\n",
    "\n",
    "# ---------- Apply global UI filters ----------\n",
    "_selected_tiers = set()\n",
    "if filters.get(\"price_tiers\"):\n",
    "    _selected_tiers.update([canon_tier_name(t) for t in filters[\"price_tiers\"] if t])\n",
    "if filters.get(\"price_tier_bucket\"):\n",
    "    _selected_tiers.add(canon_tier_name(filters[\"price_tier_bucket\"]))\n",
    "\n",
    "if _selected_tiers:\n",
    "    stock_df = stock_df[stock_df['price_tier'].isin(_selected_tiers)].copy()\n",
    "\n",
    "_selected_loyalties = {str(x).strip().lower() for x in (filters.get(\"loyalty_levels\") or []) if str(x).strip()}\n",
    "if _selected_loyalties and 'loyalty_level' in seg_df.columns:\n",
    "    seg_df = seg_df[seg_df['loyalty_level'].str.lower().isin(_selected_loyalties)].copy()\n",
    "\n",
    "last_stock_flag    = bool(filters.get(\"last_stock\", False))\n",
    "seasonality_flag   = bool(filters.get(\"seasonality_boost\", False))\n",
    "selected_type      = filters.get(\"wine_type\", None)\n",
    "selected_size_raw  = filters.get(\"bottle_size\", None)\n",
    "try:\n",
    "    selected_size_ml = int(selected_size_raw) if (selected_size_raw is not None and str(selected_size_raw).isdigit()) else None\n",
    "except Exception:\n",
    "    selected_size_ml = None\n",
    "\n",
    "# ---------- Weekday policies ----------\n",
    "WEEKDAY_OCCASION = {\n",
    "    \"Monday\": \"Casual\", \"Tuesday\": \"Casual\", \"Wednesday\": \"Dinner\",\n",
    "    \"Thursday\": \"Dinner\", \"Friday\": \"Party\", \"Saturday\": \"Gifting\", \"Sunday\": \"Dinner\"\n",
    "}\n",
    "WEEKDAY_TIERS = {\n",
    "    \"Monday\":   [\"Budget\", \"Mid-range\", \"Premium\"],\n",
    "    \"Tuesday\":  [\"Mid-range\", \"Premium\"],\n",
    "    \"Wednesday\":[\"Premium\", \"Luxury\"],\n",
    "    \"Thursday\": [\"Premium\", \"Luxury\"],\n",
    "    \"Friday\":   [\"Luxury\", \"Ultra Luxury\"],\n",
    "    \"Saturday\": [\"Luxury\", \"Ultra Luxury\"],\n",
    "    \"Sunday\":   [\"Budget\", \"Premium\", \"Luxury\"],\n",
    "}\n",
    "\n",
    "# ---------- Selector ----------\n",
    "def get_seasonal_wines_modular(stock_df, seg_df, last_stock_flag=False, seasonality=False,\n",
    "                               selected_type=None, selected_size_ml=None, num_per_day=5):\n",
    "    df = stock_df.copy()\n",
    "\n",
    "    # Type filter\n",
    "    if selected_type and str(selected_type).strip().lower() not in (\"all\", \"\"):\n",
    "        df = df[df['full_type'].astype(str).str.casefold() == str(selected_type).casefold()]\n",
    "\n",
    "    # Size filter (ml)\n",
    "    if selected_size_ml is not None and not np.isnan(selected_size_ml):\n",
    "        df = df[np.isclose(pd.to_numeric(df['bottle_size_ml'], errors='coerce'), selected_size_ml, equal_nan=False)]\n",
    "\n",
    "    # Seasonality: within ~last year → next week window (based on OMT last offer date)\n",
    "    if seasonality:\n",
    "        last_year = datetime.today() - timedelta(days=365)\n",
    "        next_week = last_year + timedelta(days=7)\n",
    "        df['OMT last offer date'] = pd.to_datetime(df.get('OMT last offer date', pd.NaT), errors='coerce')\n",
    "        df = df[df['OMT last offer date'].between(last_year, next_week)]\n",
    "\n",
    "    # Stock filters\n",
    "    if last_stock_flag:\n",
    "        df = df[df['stock'] < 10]\n",
    "    df = df[df['stock'] >= 3]\n",
    "\n",
    "    # Segment match score (region+type)\n",
    "    seg = seg_df[['region_group','full_type']].dropna()\n",
    "    def match_score(row):\n",
    "        return int(((seg['region_group'] == row['region_group']) & (seg['full_type'] == row['full_type'])).sum()) if not seg.empty else 0\n",
    "    df['segment_score'] = df.apply(match_score, axis=1) if not df.empty else 0\n",
    "\n",
    "    ultra_cap = 2\n",
    "    ultra_used = 0\n",
    "\n",
    "    calendar = {}\n",
    "\n",
    "    for day in DAYS:\n",
    "        allowed_tiers = [canon_tier_name(t) for t in WEEKDAY_TIERS.get(day, [])]\n",
    "        occasion = WEEKDAY_OCCASION.get(day, \"Casual\")\n",
    "\n",
    "        pool = df.copy()\n",
    "\n",
    "        # Step 1: match occasion + tier\n",
    "        cand = pool[(pool['occasion'].astype(str).str.casefold() == occasion.casefold()) &\n",
    "                    (pool['price_tier'].isin(allowed_tiers))]\n",
    "\n",
    "        # Ultra cap per week\n",
    "        if ultra_used >= ultra_cap:\n",
    "            cand = cand[cand['price_tier'] != \"Ultra Luxury\"]\n",
    "\n",
    "        sel = cand.sort_values(['segment_score','stock'], ascending=[False, False]).head(num_per_day)\n",
    "\n",
    "        # Step 2: relax occasion\n",
    "        if sel.empty:\n",
    "            cand2 = pool[pool['price_tier'].isin(allowed_tiers)]\n",
    "            if ultra_used >= ultra_cap:\n",
    "                cand2 = cand2[cand2['price_tier'] != \"Ultra Luxury\"]\n",
    "            sel = cand2.sort_values(['segment_score','stock'], ascending=[False, False]).head(num_per_day)\n",
    "\n",
    "        # Step 3: seasonal fallback\n",
    "        if sel.empty:\n",
    "            seasonal_only = pool.copy()\n",
    "            seasonal_only['OMT last offer date'] = pd.to_datetime(seasonal_only.get('OMT last offer date', pd.NaT), errors='coerce')\n",
    "            last_year = datetime.today() - timedelta(days=365)\n",
    "            next_week = last_year + timedelta(days=7)\n",
    "            fallback = seasonal_only[seasonal_only['OMT last offer date'].between(last_year, next_week)]\n",
    "            sel = fallback.sort_values('stock', ascending=False).head(num_per_day)\n",
    "\n",
    "        # Step 4: top stocked fallback\n",
    "        if sel.empty:\n",
    "            sel = pool.sort_values('stock', ascending=False).head(num_per_day)\n",
    "\n",
    "        # track ultra usage\n",
    "        if not sel.empty and \"Ultra Luxury\" in sel['price_tier'].values:\n",
    "            ultra_used = min(ultra_cap, ultra_used + (sel['price_tier'].eq(\"Ultra Luxury\").sum()))\n",
    "\n",
    "        calendar[day] = sel\n",
    "\n",
    "    return calendar\n",
    "\n",
    "# ---------- UI mapping ----------\n",
    "def to_ui_item(row: pd.Series) -> dict:\n",
    "    return {\n",
    "        'id': str(row.get('id', '')),\n",
    "        'wine': row.get('wine', 'Unknown'),\n",
    "        'name': row.get('wine', 'Unknown'),\n",
    "        'vintage': str(row.get('vintage', 'NV')) if pd.notna(row.get('vintage', '')) else 'NV',\n",
    "        'full_type': row.get('full_type', 'Unknown'),\n",
    "        'region_group': row.get('region_group', 'Unknown'),\n",
    "        'stock': int(pd.to_numeric(row.get('stock', 0), errors='coerce') if row.get('stock', 0) is not None else 0),\n",
    "        'price_tier': canon_tier_name(row.get('price_tier', '')),\n",
    "        'price_tier_id': tier_id_from_name(row.get('price_tier', '')),\n",
    "        'match_quality': row.get('match_quality', 'Auto'),\n",
    "        'avg_cpi_score': float(pd.to_numeric(row.get('avg_cpi_score', 0), errors='coerce')) if pd.notna(row.get('avg_cpi_score', np.nan)) else 0.0,\n",
    "        'locked': bool(row.get('locked', False))\n",
    "    }\n",
    "\n",
    "def get_ui_data(top_n_recs: pd.DataFrame, weekly_wines: dict, top_wine_counts: pd.DataFrame,\n",
    "                client_prefs: pd.DataFrame, locked_df: pd.DataFrame | None = None):\n",
    "    calendar_data = {d: [] for d in DAYS}\n",
    "\n",
    "    # 1) Locked first\n",
    "    if isinstance(locked_df, pd.DataFrame) and not locked_df.empty:\n",
    "        for day in DAYS:\n",
    "            for _, r in locked_df.loc[locked_df['day']==day].iterrows():\n",
    "                calendar_data[day].append(to_ui_item(r))\n",
    "\n",
    "    # 2) Auto picks (skip duplicates)\n",
    "    for day, df_day in (weekly_wines or {}).items():\n",
    "        if isinstance(df_day, pd.DataFrame) and not df_day.empty:\n",
    "            have = {str(it.get(\"id\",\"\")) for it in calendar_data[day]}\n",
    "            for _, r in df_day.iterrows():\n",
    "                rid = str(r.get(\"id\",\"\"))\n",
    "                if rid and rid not in have:\n",
    "                    calendar_data[day].append(to_ui_item(r))\n",
    "                    have.add(rid)\n",
    "\n",
    "    # CPI preview (avg of attached)\n",
    "    cpi_score = 0.0\n",
    "    try:\n",
    "        flat = [it.get(\"avg_cpi_score\", 0) for arr in calendar_data.values() for it in arr]\n",
    "        if flat:\n",
    "            cpi_score = float(pd.to_numeric(pd.Series(flat), errors=\"coerce\").fillna(0).mean())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'weekly_calendar': calendar_data,\n",
    "        'cpi_score': round(cpi_score, 2),\n",
    "        'client_prefs': client_prefs.to_dict('records'),\n",
    "        'top_recommendations': []\n",
    "    }\n",
    "\n",
    "# ---------- Build weekly selection ----------\n",
    "IRON_DATA_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "weekly_wines = get_seasonal_wines_modular(\n",
    "    stock_df, seg_df,\n",
    "    last_stock_flag=last_stock_flag,\n",
    "    seasonality=seasonality_flag,\n",
    "    selected_type=selected_type,\n",
    "    selected_size_ml=selected_size_ml,\n",
    "    num_per_day=NUM_SLOTS\n",
    ")\n",
    "\n",
    "ui_output_data = get_ui_data(\n",
    "    top_n_recs=pd.DataFrame(), \n",
    "    weekly_wines=weekly_wines, \n",
    "    top_wine_counts=pd.DataFrame(), \n",
    "    client_prefs=client_pref_df, \n",
    "    locked_df=locked_df\n",
    ")\n",
    "\n",
    "# Normalize & cap to NUM_SLOTS (keeps locked-first order due to append sequence)\n",
    "calendar_norm = {d: (ui_output_data['weekly_calendar'].get(d, []) or [])[:NUM_SLOTS] for d in DAYS}\n",
    "\n",
    "# ---------- Write artifacts ----------\n",
    "(IRON_DATA_PATH / \"weekly_campaign_schedule.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "# Legacy (week only)\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{sel_week}.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "# UI-preferred artifact\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.ui.json\").write_text(\n",
    "    json.dumps({\"weekly_calendar\": calendar_norm}, indent=2), encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# PKLs\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / \"weekly_campaign_schedule.pkl\")\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.pkl\")\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{sel_week}.pkl\")\n",
    "\n",
    "# Leads (present if previous logic produced it; otherwise sane default)\n",
    "leads_payload = ui_output_data.get(\"leads_campaigns\", {\"TueWed\": [], \"ThuFri\": []})\n",
    "(IRON_DATA_PATH / \"leads_campaigns.json\").write_text(json.dumps(leads_payload, indent=2), encoding=\"utf-8\")\n",
    "(IRON_DATA_PATH / f\"leads_campaigns_{sel_year}_week_{sel_week}.json\").write_text(json.dumps(leads_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Index (nested by year→week)\n",
    "index_path = IRON_DATA_PATH / \"schedule_index.json\"\n",
    "try:\n",
    "    idx = json.loads(index_path.read_text(encoding=\"utf-8\")) if index_path.exists() else {}\n",
    "except Exception:\n",
    "    idx = {}\n",
    "idx.setdefault(str(sel_year), {})[str(sel_week)] = {\n",
    "    \"json\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.json\",\n",
    "    \"json_ui\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.ui.json\",\n",
    "    \"pkl\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.pkl\",\n",
    "    \"leads_json\": f\"leads_campaigns_{sel_year}_week_{sel_week}.json\",\n",
    "    \"updated_at\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "idx[\"_latest_year\"] = str(sel_year)\n",
    "idx[\"_latest_week\"] = str(sel_week)\n",
    "(IRON_DATA_PATH / \"schedule_index.json\").write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Saved year+week UI files for {sel_year}-W{sel_week} in {IRON_DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05c215e-37f1-42f5-a12b-1f7d8d954186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:33.123778Z",
     "iopub.status.busy": "2025-09-05T06:55:33.123778Z",
     "iopub.status.idle": "2025-09-05T06:55:33.389389Z",
     "shell.execute_reply": "2025-09-05T06:55:33.387385Z"
    },
    "papermill": {
     "duration": 0.26661,
     "end_time": "2025-09-05T06:55:33.390388",
     "exception": false,
     "start_time": "2025-09-05T06:55:33.123778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ build_weekly_leads failed: 'last_campaign_date'\n",
      "📣 Leads saved for 2025-W36\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 9: Build weekly \"Leads\" campaigns (Tue–Wed & Thu–Fri) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Ensure IRON_DATA_PATH exists (fallback to OUTPUT_PATH or default OneDrive path)\n",
    "if 'IRON_DATA_PATH' not in globals():\n",
    "    if 'OUTPUT_PATH' in globals():\n",
    "        IRON_DATA_PATH = OUTPUT_PATH\n",
    "    else:\n",
    "        IRON_DATA_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip().lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    if not s: return \"\"\n",
    "    if \"ultra\" in s: return \"Ultra Luxury\"\n",
    "    if \"luxury\" in s: return \"Luxury\"\n",
    "    if \"premium\" in s: return \"Premium\"\n",
    "    if \"mid\" in s: return \"Mid-range\"\n",
    "    if \"budget\" in s or \"<\" in s or \"cheap\" in s: return \"Budget\"\n",
    "    return s.title()\n",
    "\n",
    "def _load_inventory(iron_path: Path) -> pd.DataFrame:\n",
    "    # Prefer the most enriched inventory if present\n",
    "    candidates = [\n",
    "        iron_path / \"stock_df_with_seasonality.pkl\",\n",
    "        iron_path / \"stock_df_final.pkl\",\n",
    "    ]\n",
    "    src = next((p for p in candidates if p.exists()), None)\n",
    "    if not src:\n",
    "        return pd.DataFrame(columns=[\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"avg_cpi_score\"])\n",
    "    df = pd.read_pickle(src)\n",
    "\n",
    "    # Normalize columns we rely on\n",
    "    for c in (\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"price\",\"avg_cpi_score\"):\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df[\"id\"] = df[\"id\"].astype(\"string\").fillna(\"\").str.replace(r\"\\.0$\", \"\", regex=True).str.strip()\n",
    "    df[\"wine\"] = df[\"wine\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"vintage\"] = df[\"vintage\"].astype(\"string\").fillna(\"\").str.strip().replace({\"\": \"NV\",\"nan\":\"NV\"})\n",
    "    df[\"full_type\"] = df[\"full_type\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"region_group\"] = df[\"region_group\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"stock\"] = pd.to_numeric(df[\"stock\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Price tier: prefer provided; else derive from price if present\n",
    "    df[\"price_tier\"] = df[\"price_tier\"].astype(\"string\").fillna(\"\").map(_canon_tier_name)\n",
    "    if df[\"price_tier\"].eq(\"\").any() and \"price\" in df.columns:\n",
    "        price_num = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "        def _bucket(p):\n",
    "            if pd.isna(p): return \"\"\n",
    "            if p < 50: return \"Budget\"\n",
    "            if p < 100: return \"Mid-range\"\n",
    "            if p < 200: return \"Premium\"\n",
    "            if p < 500: return \"Luxury\"\n",
    "            return \"Ultra Luxury\"\n",
    "        mask = df[\"price_tier\"].eq(\"\")\n",
    "        df.loc[mask, \"price_tier\"] = price_num.map(_bucket)\n",
    "\n",
    "    # keep only rows with something to offer\n",
    "    df = df[df[\"stock\"] > 0].copy()\n",
    "    return df\n",
    "\n",
    "def _attach_last_campaign(inv: pd.DataFrame, history: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach last_campaign_date to inventory rows. Uses history_df when provided;\n",
    "    otherwise falls back to OUTPUT/IRON campaign_index.json if available.\n",
    "    \"\"\"\n",
    "    inv = inv.copy()\n",
    "    inv[\"last_campaign_date\"] = pd.NaT\n",
    "\n",
    "    # 1) If a rich history_df is available\n",
    "    if history is not None and not history.empty and {\"id\",\"wine\",\"vintage\",\"schedule_dt\"}.issubset(history.columns):\n",
    "        hist = history.copy()\n",
    "        hist[\"id\"] = hist[\"id\"].astype(\"string\").fillna(\"\").str.replace(r\"\\.0$\",\"\",regex=True).str.strip()\n",
    "        hist[\"wine\"] = hist[\"wine\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "        hist[\"vintage\"] = hist[\"vintage\"].astype(\"string\").fillna(\"\").str.strip().replace({\"\": \"NV\",\"nan\":\"NV\"})\n",
    "        hist[\"schedule_dt\"] = pd.to_datetime(hist[\"schedule_dt\"], errors=\"coerce\")\n",
    "\n",
    "        last_seen = (\n",
    "            hist.dropna(subset=[\"schedule_dt\"])\n",
    "                .groupby([\"id\",\"wine\",\"vintage\"], dropna=False)[\"schedule_dt\"]\n",
    "                .max()\n",
    "                .reset_index()\n",
    "                .rename(columns={\"schedule_dt\":\"last_campaign_date\"})\n",
    "        )\n",
    "        return inv.merge(last_seen, how=\"left\", on=[\"id\",\"wine\",\"vintage\"])\n",
    "\n",
    "    # 2) Fallback: campaign_index.json (fast, id-based)\n",
    "    idx_path = IRON_DATA_PATH / \"campaign_index.json\"\n",
    "    if not idx_path.exists() and 'OUTPUT_PATH' in globals():\n",
    "        alt = OUTPUT_PATH / \"campaign_index.json\"\n",
    "        if alt.exists():\n",
    "            idx_path = alt\n",
    "\n",
    "    if idx_path.exists():\n",
    "        try:\n",
    "            ci = json.loads(idx_path.read_text(encoding=\"utf-8\"))\n",
    "            by_id = ci.get(\"by_id\", {})\n",
    "            id_map = {str(k): v.get(\"last_campaign_date\") for k, v in by_id.items()}\n",
    "            inv[\"last_campaign_date\"] = inv[\"id\"].map(id_map)\n",
    "            inv[\"last_campaign_date\"] = pd.to_datetime(inv[\"last_campaign_date\"], errors=\"coerce\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return inv\n",
    "\n",
    "def _pick_one(df, sort_cols, asc):\n",
    "    if df.empty: return pd.DataFrame(columns=df.columns)\n",
    "    return df.sort_values(sort_cols, ascending=asc, na_position=\"last\").head(1)\n",
    "\n",
    "def _pick_varied(df, used_ids, k=2):\n",
    "    pool = df[~df[\"id\"].isin(used_ids)].copy()\n",
    "    picks = []\n",
    "\n",
    "    # prefer different full_type across picks if possible\n",
    "    type_order = [\"Sparkling\",\"White\",\"Rosé\",\"Rose\",\"Red\",\"Dessert\"]\n",
    "    for t in type_order:\n",
    "        cand = pool[pool[\"full_type\"].str.contains(t, case=False, na=False)]\n",
    "        got = _pick_one(cand, [\"stock\",\"last_campaign_date\"], [False, True])\n",
    "        if not got.empty:\n",
    "            picks.append(got)\n",
    "            pool = pool[~pool[\"id\"].isin(got[\"id\"])]\n",
    "        if len(picks) >= k:\n",
    "            break\n",
    "\n",
    "    # fallback if we didn’t get enough\n",
    "    if len(picks) < k and not pool.empty:\n",
    "        extra = pool.sort_values([\"stock\",\"last_campaign_date\"], ascending=[False, True], na_position=\"last\").head(k - len(picks))\n",
    "        if not extra.empty:\n",
    "            picks.append(extra)\n",
    "\n",
    "    return pd.concat(picks, ignore_index=True) if picks else pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def build_weekly_leads(iron_path: Path, history_df: pd.DataFrame | None) -> dict:\n",
    "    inv = _load_inventory(iron_path)\n",
    "    inv = _attach_last_campaign(inv, history_df)\n",
    "\n",
    "    # 1) Budget (high stock)\n",
    "    budget = inv[inv[\"price_tier\"].eq(\"Budget\")]\n",
    "    lead_budget = _pick_one(budget, [\"stock\",\"last_campaign_date\"], [False, True])\n",
    "\n",
    "    # 2) Ultra Luxury (low stock)\n",
    "    ultra = inv[inv[\"price_tier\"].eq(\"Ultra Luxury\")]\n",
    "    lead_ultra = _pick_one(ultra, [\"stock\",\"last_campaign_date\"], [True, True])\n",
    "\n",
    "    # 3–4) Two varied items (not already chosen)\n",
    "    used_ids = pd.concat([lead_budget, lead_ultra])[\"id\"].astype(str).tolist() if not pd.concat([lead_budget, lead_ultra]).empty else []\n",
    "    varied = _pick_varied(inv, used_ids, k=2)\n",
    "\n",
    "    leads_df = pd.concat([lead_budget, lead_ultra, varied], ignore_index=True)\n",
    "    if leads_df.empty:\n",
    "        return {\"TueWed\": [], \"ThuFri\": []}\n",
    "\n",
    "    def _row_to_item(r):\n",
    "        return {\n",
    "            \"id\": str(r.get(\"id\",\"\")),\n",
    "            \"wine\": r.get(\"wine\") or \"Unknown\",\n",
    "            \"name\": r.get(\"wine\") or \"Unknown\",\n",
    "            \"vintage\": str(r.get(\"vintage\",\"NV\")) if str(r.get(\"vintage\",\"\")).strip() else \"NV\",\n",
    "            \"full_type\": r.get(\"full_type\") or \"Unknown\",\n",
    "            \"region_group\": r.get(\"region_group\") or \"Unknown\",\n",
    "            \"stock\": int(pd.to_numeric(r.get(\"stock\", 0), errors=\"coerce\") or 0),\n",
    "            \"price_tier\": r.get(\"price_tier\") or \"\",\n",
    "            \"match_quality\": \"Lead\",\n",
    "            \"avg_cpi_score\": float(pd.to_numeric(r.get(\"avg_cpi_score\", 0), errors=\"coerce\") or 0),\n",
    "            \"last_campaign_date\": (\n",
    "                r.get(\"last_campaign_date\").isoformat()\n",
    "                if pd.notna(r.get(\"last_campaign_date\")) else None\n",
    "            ),\n",
    "            # UI helpers:\n",
    "            \"locked\": False,\n",
    "            \"span_days\": 2,             # 2-day “Leads” box\n",
    "            \"campaign_tag\": \"leads\"\n",
    "        }\n",
    "\n",
    "    items = [ _row_to_item(r) for _, r in leads_df.iterrows() ]\n",
    "    # Same set in both boxes (simple + consistent)\n",
    "    return {\"TueWed\": items, \"ThuFri\": items}\n",
    "\n",
    "# Get history_df if available; otherwise pass None (we’ll fall back to campaign_index.json)\n",
    "_hist = history_df if 'history_df' in globals() else None\n",
    "try:\n",
    "    leads_campaigns = build_weekly_leads(IRON_DATA_PATH, _hist)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ build_weekly_leads failed: {e}\")\n",
    "    leads_campaigns = {\"TueWed\": [], \"ThuFri\": []}\n",
    "\n",
    "# Persist year+week leads alongside generic (for fast API fetches)\n",
    "try:\n",
    "    y, w = int(calendar_year), int(week_number)\n",
    "except Exception:\n",
    "    y, w = datetime.now().year, datetime.now().isocalendar().week\n",
    "\n",
    "lp = IRON_DATA_PATH\n",
    "lp.mkdir(parents=True, exist_ok=True)\n",
    "(lp / \"leads_campaigns.json\").write_text(json.dumps(leads_campaigns, indent=2), encoding=\"utf-8\")\n",
    "(lp / f\"leads_campaigns_{y}_week_{w}.json\").write_text(json.dumps(leads_campaigns, indent=2), encoding=\"utf-8\")\n",
    "print(f\"📣 Leads saved for {y}-W{w}\")\n",
    "\n",
    "# Attach to in-memory UI bundle if present\n",
    "if \"ui_output_data\" not in locals():\n",
    "    ui_output_data = {}\n",
    "ui_output_data[\"leads_campaigns\"] = leads_campaigns\n",
    "\n",
    "# (Removed undefined atomic_write_* calls and the incorrect 'leads_payload' reference.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc670ad4-47b7-4ad8-b3dc-b644a7e0b9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:33.391388Z",
     "iopub.status.busy": "2025-09-05T06:55:33.391388Z",
     "iopub.status.idle": "2025-09-05T06:55:33.442586Z",
     "shell.execute_reply": "2025-09-05T06:55:33.442586Z"
    },
    "papermill": {
     "duration": 0.05508,
     "end_time": "2025-09-05T06:55:33.445468",
     "exception": false,
     "start_time": "2025-09-05T06:55:33.390388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UI JSON and PKL saved atomically for week 36\n",
      "📅 Days in calendar: ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
      "🧮 Slots per day: {'Monday': 5, 'Tuesday': 5, 'Wednesday': 5, 'Thursday': 5, 'Friday': 5, 'Saturday': 5, 'Sunday': 5}\n",
      "🎯 CPI score: 0.06\n",
      "📣 Leads (counts): {'TueWed': 0, 'ThuFri': 0}\n",
      "⚠️ update_status failed: update_status() got an unexpected keyword argument 'progress'\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 10: Flask handoff & week outputs (atomic + robust) ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Safety Check ===\n",
    "if \"ui_output_data\" not in locals():\n",
    "    raise RuntimeError(\"❌ ui_output_data not found. Make sure the previous cell populated it.\")\n",
    "\n",
    "# === Status bridge (prefer package import; fallback to none) ===\n",
    "try:\n",
    "    from utils.notebook_status import update_status  # when run by Flask\n",
    "except Exception:\n",
    "    try:\n",
    "        from notebook_status import update_status     # when run locally\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ update_status unavailable: {e}\")\n",
    "        update_status = None\n",
    "\n",
    "# === Paths / Week ===\n",
    "IRON_DATA_PATH = Path(\n",
    "    globals().get(\"OUTPUT_PATH\")\n",
    "    or globals().get(\"output_path\")\n",
    "    or os.getenv(\"OUTPUT_PATH\")\n",
    "    or os.getenv(\"IRON_DATA\")\n",
    "    or (Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\")\n",
    ")\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    week_number = int(globals().get(\"week_number\", os.getenv(\"WEEK_NUMBER\", datetime.now().isocalendar().week)))\n",
    "except Exception:\n",
    "    week_number = datetime.now().isocalendar().week\n",
    "\n",
    "# === Helpers ===\n",
    "DAYS = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "TIER_MAP_NAME = {\n",
    "    \"budget\":\"Budget\",\"entry\":\"Budget\",\n",
    "    \"mid\":\"Mid-range\",\"mid_range\":\"Mid-range\",\"mid-range\":\"Mid-range\",\n",
    "    \"premium\":\"Premium\",\n",
    "    \"luxury\":\"Luxury\",\n",
    "    \"ultra\":\"Ultra Luxury\",\"ultra_luxury\":\"Ultra Luxury\",\"ultra-luxury\":\"Ultra Luxury\",\"ultra luxury\":\"Ultra Luxury\",\n",
    "}\n",
    "TIER_MAP_ID = {\"Budget\":\"budget\",\"Mid-range\":\"mid\",\"Premium\":\"premium\",\"Luxury\":\"luxury\",\"Ultra Luxury\":\"ultra\"}\n",
    "\n",
    "def canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s: return \"\"\n",
    "    key = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return TIER_MAP_NAME.get(key, s)\n",
    "\n",
    "def tier_id_from_name(name: str) -> str:\n",
    "    return TIER_MAP_ID.get(canon_tier_name(name), \"\")\n",
    "\n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(pd.to_numeric(x, errors=\"coerce\").fillna(default))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def atomic_write_text(path: Path, text: str):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    tmp.write_text(text, encoding=\"utf-8\")\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def atomic_write_pkl(path: Path, obj):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    pd.to_pickle(obj, tmp)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def ensure_calendar_schema(calendar: dict) -> dict:\n",
    "    \"\"\"Guarantee days exist, coerce items and cap to 5 per day.\"\"\"\n",
    "    out = {d: [] for d in DAYS}\n",
    "    if not isinstance(calendar, dict):\n",
    "        return out\n",
    "    for day in DAYS:\n",
    "        items = calendar.get(day, []) or []\n",
    "        norm_items = []\n",
    "        for it in items:\n",
    "            it = it or {}\n",
    "            price_tier = canon_tier_name(it.get(\"price_tier\", it.get(\"price_tier_bucket\", \"\")))\n",
    "            norm_items.append({\n",
    "                \"id\":              str(it.get(\"id\", \"\")),\n",
    "                \"wine\":            it.get(\"wine\") or it.get(\"name\") or \"Unknown\",\n",
    "                \"name\":            it.get(\"name\") or it.get(\"wine\") or \"Unknown\",\n",
    "                \"vintage\":         str(it.get(\"vintage\", \"NV\")) if str(it.get(\"vintage\",\"\")).strip() else \"NV\",\n",
    "                \"full_type\":       it.get(\"full_type\", \"Unknown\"),\n",
    "                \"region_group\":    it.get(\"region_group\", \"Unknown\"),\n",
    "                \"stock\":           _to_int(it.get(\"stock\", it.get(\"stock_count\", 0))),\n",
    "                \"price_tier\":      price_tier,\n",
    "                \"price_tier_id\":   tier_id_from_name(price_tier),\n",
    "                \"match_quality\":   it.get(\"match_quality\", \"Auto\"),\n",
    "                \"avg_cpi_score\":   float(pd.to_numeric(it.get(\"avg_cpi_score\", it.get(\"cpi_score\", 0)), errors=\"coerce\") or 0),\n",
    "                \"locked\":          bool(it.get(\"locked\", False)),\n",
    "            })\n",
    "        out[day] = norm_items[:5]\n",
    "    return out\n",
    "\n",
    "# === Normalize payload from Cell 8/9 ===\n",
    "calendar_raw = ui_output_data.get(\"weekly_calendar\", {})\n",
    "calendar_norm = ensure_calendar_schema(calendar_raw)\n",
    "\n",
    "# Leads payload produced in Cell 9\n",
    "leads_payload = ui_output_data.get(\"leads_campaigns\", {\"TueWed\": [], \"ThuFri\": []})\n",
    "\n",
    "# Optional summary for status callback\n",
    "summary = {\n",
    "    \"week_number\": week_number,\n",
    "    \"updated_at\": _now_iso(),\n",
    "    \"days\": {d: len(calendar_norm.get(d, [])) for d in DAYS},\n",
    "    \"cpi_score\": ui_output_data.get(\"cpi_score\", None),\n",
    "    \"top_recommendations\": ui_output_data.get(\"top_recommendations\", []),\n",
    "    \"leads_counts\": {k: len(v or []) for k, v in (leads_payload or {}).items()}\n",
    "}\n",
    "\n",
    "# === Save (atomic) ===\n",
    "json_generic = IRON_DATA_PATH / \"weekly_campaign_schedule.json\"\n",
    "pkl_generic  = IRON_DATA_PATH / \"weekly_campaign_schedule.pkl\"\n",
    "json_week    = IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{week_number}.json\"\n",
    "pkl_week     = IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{week_number}.pkl\"\n",
    "\n",
    "# Dedicated leads files\n",
    "leads_json_generic = IRON_DATA_PATH / \"leads_campaigns.json\"\n",
    "leads_json_week    = IRON_DATA_PATH / f\"leads_campaigns_week_{week_number}.json\"\n",
    "\n",
    "index_path   = IRON_DATA_PATH / \"schedule_index.json\"\n",
    "\n",
    "try:\n",
    "    # Write calendar artifacts\n",
    "    atomic_write_text(json_generic, json.dumps(calendar_norm, indent=4))\n",
    "    atomic_write_text(json_week,    json.dumps(calendar_norm, indent=4))\n",
    "    atomic_write_pkl(pkl_generic, calendar_norm)\n",
    "    atomic_write_pkl(pkl_week,    calendar_norm)\n",
    "\n",
    "    # Write leads artifacts\n",
    "    atomic_write_text(leads_json_generic, json.dumps(leads_payload, indent=2))\n",
    "    atomic_write_text(leads_json_week,    json.dumps(leads_payload, indent=2))\n",
    "\n",
    "    # Maintain a simple index for the server\n",
    "    try:\n",
    "        idx = json.loads(index_path.read_text(encoding=\"utf-8\")) if index_path.exists() else {}\n",
    "    except Exception:\n",
    "        idx = {}\n",
    "    idx[str(week_number)] = {\n",
    "        \"json\": json_week.name,\n",
    "        \"pkl\":  pkl_week.name,\n",
    "        \"leads_json\": leads_json_week.name,\n",
    "        \"updated_at\": summary[\"updated_at\"]\n",
    "    }\n",
    "    idx[\"_latest\"] = str(week_number)\n",
    "    atomic_write_text(index_path, json.dumps(idx, indent=2))\n",
    "\n",
    "    print(f\"✅ UI JSON and PKL saved atomically for week {week_number}\")\n",
    "    print(\"📅 Days in calendar:\", list(calendar_norm.keys()))\n",
    "    print(\"🧮 Slots per day:\", {d: len(calendar_norm[d]) for d in DAYS})\n",
    "    print(\"🎯 CPI score:\", ui_output_data.get('cpi_score', 'N/A'))\n",
    "    print(\"📣 Leads (counts):\", summary[\"leads_counts\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while saving UI files: {e}\")\n",
    "\n",
    "# === Report back to Flask UI (if available) ===\n",
    "if callable(update_status):\n",
    "    try:\n",
    "        _nb_name = globals().get(\"NOTEBOOK_NAME\") or os.getenv(\"NOTEBOOK_NAME\") or \"AVU_schedule_only.ipynb\"\n",
    "        update_status(\n",
    "            progress=100,\n",
    "            message=\"✅ Notebook finished.\",\n",
    "            state=\"completed\",\n",
    "            done=True,\n",
    "            notebook=_nb_name,\n",
    "            meta=summary\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ update_status failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5810577a-4a46-410c-b2c8-868f5df22a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T06:55:33.446471Z",
     "iopub.status.busy": "2025-09-05T06:55:33.446471Z",
     "iopub.status.idle": "2025-09-05T06:55:33.952756Z",
     "shell.execute_reply": "2025-09-05T06:55:33.949752Z"
    },
    "papermill": {
     "duration": 0.508293,
     "end_time": "2025-09-05T06:55:33.953761",
     "exception": false,
     "start_time": "2025-09-05T06:55:33.445468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Power BI layout files saved:\n",
      "   • C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi_wine_arrow_layout.xlsx\n",
      "   • C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi_wine_arrow_layout.csv\n",
      "🧭 Rows: 731 | Columns: 15\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 11: Generate a file readable by Power BI (from normalized stock) \n",
    "#--- then PowerBI visualizion will be on the UI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Get the normalized stock frame\n",
    "try:\n",
    "    df = stock_df.copy()  # from Cell 3\n",
    "except NameError:\n",
    "    # Fallback: read from the artifact saved in Cell 3\n",
    "    df_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    if not df_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ Could not find {df_path}. Run Cell 3 first.\")\n",
    "    df = pd.read_pickle(df_path)\n",
    "\n",
    "# 2) Ensure required columns exist\n",
    "for col in [\"price_tier\", \"CHF Price\", \"vintage\", \"id\", \"wine\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Normalize numeric CHF price\n",
    "df[\"CHF Price\"] = pd.to_numeric(df[\"CHF Price\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Canonicalize price tiers + add stable ID\n",
    "def price_tier_from_price(price):\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p < 50:   return \"Budget\"\n",
    "    if p < 100:  return \"Mid-range\"\n",
    "    if p < 200:  return \"Premium\"\n",
    "    if p < 500:  return \"Luxury\"\n",
    "    return \"Ultra Luxury\"\n",
    "\n",
    "def canon_tier_name(x):\n",
    "    s = str(x or \"\").strip().lower()\n",
    "    if not s: return \"\"\n",
    "    if \"ultra\" in s: return \"Ultra Luxury\"\n",
    "    if \"luxury\" in s: return \"Luxury\"\n",
    "    if \"premium\" in s: return \"Premium\"\n",
    "    if \"mid\" in s: return \"Mid-range\"\n",
    "    if \"budget\" in s: return \"Budget\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def tier_id(name):\n",
    "    m = {\n",
    "        \"Budget\":\"budget\",\n",
    "        \"Mid-range\":\"mid\",\n",
    "        \"Premium\":\"premium\",\n",
    "        \"Luxury\":\"luxury\",\n",
    "        \"Ultra Luxury\":\"ultra\",\n",
    "        \"Unknown\":\"\"\n",
    "    }\n",
    "    return m.get(name, \"\")\n",
    "\n",
    "# Fill missing/blank tiers from price, then canonicalize\n",
    "df[\"price_tier\"] = df[\"price_tier\"].where(df[\"price_tier\"].notna() & (df[\"price_tier\"].astype(str).str.strip()!=\"\"))\n",
    "df.loc[df[\"price_tier\"].isna() | (df[\"price_tier\"].astype(str).str.strip()==\"\"), \"price_tier\"] = df[\"CHF Price\"].apply(price_tier_from_price)\n",
    "df[\"Cleaned_Price_Tier\"] = df[\"price_tier\"].apply(canon_tier_name)\n",
    "df[\"price_tier_id\"] = df[\"Cleaned_Price_Tier\"].apply(tier_id)\n",
    "\n",
    "# 4) Vintage grouping (Power BI-friendly labels)\n",
    "def classify_vintage_group(vint):\n",
    "    try:\n",
    "        s = str(vint).strip().upper()\n",
    "        if s == \"NV\": return \"Non-Vintage\"\n",
    "        y = int(s)\n",
    "        cy = datetime.now().year\n",
    "        if y == cy:       return \"Current Vintage\"\n",
    "        if y == cy - 1:   return \"En Primeur\"\n",
    "        if cy - y <= 3:   return \"Young Wines\"\n",
    "        if cy - y <= 8:   return \"Mature Wines\"\n",
    "        if cy - y > 15:   return \"Old Wines\"\n",
    "        return \"Unknown\"\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Vintage_Group\"] = df[\"vintage\"].apply(classify_vintage_group)\n",
    "\n",
    "# 5) Order within each tier for triangular/scatter layout\n",
    "#    Sort by tier (alphabetical works with our canonical names) then by CHF price desc\n",
    "df = df.sort_values([\"Cleaned_Price_Tier\", \"CHF Price\"], ascending=[True, False]).reset_index(drop=True)\n",
    "\n",
    "# Tier-local index\n",
    "df[\"Tier_Index\"] = df.groupby(\"Cleaned_Price_Tier\").cumcount() + 1\n",
    "\n",
    "# Row of triangular packing (n-th triangular number inverse)\n",
    "def compute_row(idx):\n",
    "    # idx is 1-based\n",
    "    return int((8 * idx + 1) ** 0.5 - 1) // 2\n",
    "\n",
    "df[\"Row_Number\"] = df[\"Tier_Index\"].apply(compute_row)\n",
    "\n",
    "# Vertical coordinate (top-down)\n",
    "df[\"Arrow_Y\"] = -df[\"Row_Number\"]\n",
    "\n",
    "# Horizontal coordinate centered within each row\n",
    "df[\"Arrow_X_raw\"] = (\n",
    "    df[\"Tier_Index\"]\n",
    "    - (df[\"Row_Number\"] * (df[\"Row_Number\"] + 1) // 2)\n",
    "    - (df[\"Row_Number\"] / 2.0)\n",
    ")\n",
    "\n",
    "# Horizontal offsets per tier (keeps tiers separated left→right)\n",
    "tier_offsets = {\n",
    "    \"Budget\": 0,\n",
    "    \"Mid-range\": 10,\n",
    "    \"Premium\": 20,\n",
    "    \"Luxury\": 30,\n",
    "    \"Ultra Luxury\": 40\n",
    "}\n",
    "df[\"Tier_X_Offset\"] = df[\"Cleaned_Price_Tier\"].map(tier_offsets).fillna(0)\n",
    "df[\"Arrow_X\"] = df[\"Arrow_X_raw\"] + df[\"Tier_X_Offset\"]\n",
    "\n",
    "# 6) Select nice columns for Power BI (keep original too)\n",
    "cols = [\n",
    "    \"id\",\"wine\",\"vintage\",\"Vintage_Group\",\n",
    "    \"CHF Price\",\"stock\",\"region_group\",\"full_type\",\n",
    "    \"price_tier\",\"Cleaned_Price_Tier\",\"price_tier_id\",\n",
    "    \"Tier_Index\",\"Row_Number\",\"Arrow_X\",\"Arrow_Y\"\n",
    "]\n",
    "present_cols = [c for c in cols if c in df.columns]\n",
    "out_df = df[present_cols].copy()\n",
    "\n",
    "# 7) Save to IRON_DATA/OUTPUT_PATH so Power BI can pick it up\n",
    "out_xlsx = OUTPUT_PATH / \"powerbi_wine_arrow_layout.xlsx\"\n",
    "out_csv  = OUTPUT_PATH / \"powerbi_wine_arrow_layout.csv\"\n",
    "\n",
    "out_df.to_excel(out_xlsx, index=False)\n",
    "out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Power BI layout files saved:\")\n",
    "print(\"   •\", out_xlsx)\n",
    "print(\"   •\", out_csv)\n",
    "print(\"🧭 Rows:\", len(out_df), \"| Columns:\", len(out_df.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 160.914287,
   "end_time": "2025-09-05T06:55:34.749149",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks\\AVU_ignition_1.ipynb",
   "output_path": "notebooks\\executed_AVU_ignition_1.ipynb",
   "parameters": {
    "input_path": "C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\SOURCE_FILES",
    "output_path": "C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA",
    "week_number": 36
   },
   "start_time": "2025-09-05T06:52:53.834862",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}